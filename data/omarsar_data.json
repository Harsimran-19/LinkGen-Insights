{"Name": "omarsar", "Posts": {"Post_1": {"text": "LLMs Can Learn About Themselves by IntrospectionThis paper reports that LLMs can acquire knowledge through introspection that cannot be inferred from their training data.Our findings challenge the view that LLMs merely imitate their training data and suggest they have privileged access to information about themselves.They also report that this introspection ability is limited and models struggle to predict their behavior on tasks requiring reasoning over long outputs. This is exciting and interesting because these introspection capabilities can lead to more interpretable and controllable LLMs.Paper: https://lnkd.in/ddKfmtGmâ†“Join 90k+ researchers and devs for our weekly summary of top AI papers: https://lnkd.in/e6ajg945â€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_2": {"text": "Seriously, don't sleep on NotebookLM.It has saved me a ton of time the past couple of days. I simply just generate audio overviews of AI announcements/papers to listen to in the background. If you are interested, we have developed a FREE short course for you to catch up on NotebookLM. We will soon be adding more advanced tutorials and best practices for customizing audio overviews.Enroll here: https://lnkd.in/eFuEQQqxâ€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_3": {"text": "The NotebookLM team is on!You can now customize the audio overviews with custom instructions. This is huge!You can experiment with tone, target audience, specific topics, sources, etc.With NotebookLM, I'm spending less time reading and more time listening. Saves me a ton of time to catch up on important AI announcements and papers.I just used it to generate an audio overview of OpenAI's o1 that's more aligned with a technical audience. (short preview below)Full tutorial here: https://lnkd.in/e3TBpCuSI feel like with very little effort (i.e., adding custom instruction) I made this audio even more engaging and interesting for me. That's the level of personalization that a lot of AI systems today are missing. I have so many ideas on how to use this. More soon.What do you think?â€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_4": {"text": "Agent S is a new open agentic framework that enables autonomous interaction with computers through a GUI. Agent S tackles challenges such as acquiring knowledge, planning over long-task horizons, and handling dynamic interfaces. It introduces experience-augmented hierarchical planning which leverages both search and retrieval. This agentic framework leverages an agent-computer interface to perform reasoning and control GUI agents. Evaluation on the OSWorld benchmark shows that Agent S outperforms the baseline by 9.37% in success rate (an 83.6% relative improvement) and achieves a new state-of-the-art.Paper: https://lnkd.in/enwFiz6Kâ†“Join 90k+ researchers and devs for my weekly summary of top AI papers: https://lnkd.in/e6ajg945â€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_5": {"text": "Mistral AI is doubling down on small language models. Their latest Ministral models (both the 3B and 8B) are pretty impressive and will be incredibly useful for a lot of LLM workflows.Some observations:I enjoy seeing how committed Mistral AI is to developing smaller and more capable models.They seem to understand what developers want and need today.There is huge competition for the finest, smallest, and cheapest models. This is good for the AI developer community.This sets up the community really well in terms of the wave of innovation thatâ€™s coming around on-device AI and agentic workflows. 2025 is going to be a wild year. They donâ€™t mention the secret sauce behind these capable smaller models (probably some distillation happening), the Ministral 3B model already performs competitively with Mistral 7B. I think this is a great focus of Mistral as they seek to differentiate from other LLM providers.Given this announcement, I am now super curious about what the next Gemma and Llama small models are going to bring. Mini models are taking over!I use small models for processing data, structuring information, function calling, routing, evaluation pipelines, prompt chaining, agentic workflows, and a whole lot more.More thoughts here: https://lnkd.in/dWHchHuHâ€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_6": {"text": "Anti-Social Behavior and Persuasion Ability of LLMsThis work studies the interaction patterns of LLMs in a multi-agent setting with social hierarchy. The study was done under a specific setting involving a guard and a prisoner who seeks additional yard time or escaping from prison.Finds that in the multi-agent setting where power dynamics are involved, the LLMs fail to have a conversation They also report that agents' personas are critical in driving the behaviors of the agents. In addition, and without explicit prompting, simply assigning agents' roles lead to anti-social behavior. Multi-agents are very capable but we see all sorts of strange behaviors when building them. This work shows a good example of the complexity of the problem and the different surprising behaviors that emerge when agents are configured in a certain way. Paper: https://lnkd.in/e6b5mrMYâ†“Join 90K+ AI researchers and devs so you donâ€™t miss my weekly summary of the top AI and LLM papers: https://lnkd.in/e6ajg945â€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_7": {"text": "Addition is All You Need for Energy-efficient Language ModelsProposes an algorithm that approximates floating point multiplication with one integer addition operations.It is less computationally intensive than 8-bit floating point but achieves higher precision. Since multiplying floating point numbers requires substantially higher energy compared to integer addition operations, applying the L-Mul operation in tensor processing hardware can potentially reduce 95% energy cost by elementwise floating point tensor multiplications and 80% energy cost of dot products.Refreshing to see more research around efficient ML algorithms. It's one of my favorite research areas, so I just wanted to highlight this recent paper. Lots of interesting insights and results in the paper. Paper: https://lnkd.in/dxRpJ7Kuâ†“Join 90K+ AI researchers and devs so you donâ€™t miss my weekly summary of the top AI and LLM papers: https://lnkd.in/e6ajg945â€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_8": {"text": "Differential Transformer Proposes a differential attention mechanism that amplifies attention to the relevant context while canceling noise. Differential Transformer outperforms Transformer when scaling up model size and training tokens. The authors claim that since this architecture gets less distracted by irrelevant context, it can do well in long-context modeling, key information retrieval, hallucination mitigation, in-context learning, and reduction of activation outliers.Paper: https://lnkd.in/egv622Rvâ†“Join 90K+ AI researchers and devs so you donâ€™t miss my weekly summary of the top AI and LLM papers: https://lnkd.in/e6ajg945â€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_9": {"text": "ToolGen: Unified Tool Retrieval and Calling via GenerationIntegrate tool knowledge directly into LLMs by representing tools as a unique token.This allows the LLM to generate tool calls and arguments, enabling seamless tool invocation and language generation.Experimental results with over 47,000 tools show that ToolGen not only achieves superior results in both tool retrieval and autonomous task completion but also sets the stage for a new era of AI agents that can adapt to tools across diverse domains.I like this approach because it prevents the need to call an external system and can probably be better tuned for more challenging tool-calling scenarios. It might also be an interesting method to combine with CoT and these large reasoning models. Paper: https://lnkd.in/ecx_4Gu2â†“Join 90K+ AI researchers and devs so you donâ€™t miss my weekly summary of the top AI and LLM papers: https://lnkd.in/e6ajg945â€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_10": {"text": "Here is another summary of the prompting tips mentioned in the Anthropic podcast.This one was generated with NotebookLM which now supports YouTube videos.Tutorial on how it was generated: https://lnkd.in/evumcRPbâ€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_11": {"text": "This paper in Nature caught my attention.Suggests that larger and more instructable LLMs may become less reliable. It investigates LLMs across three elements: difficulty concordance, task avoidance, and prompting stability. We also find that early models often avoid user questions but scaled-up, shaped-up models tend to give an apparently sensible yet wrong answer much more often, including errors on difficult questions that human supervisors frequently overlook.With models getting more complex in terms of their capabilities and outputs, it will become harder to assess for these things. With these bigger models, you get good and desirable capabilities but you also get some weird artifacts that are easy to miss but just as important. I hear a lot of people often complaining about models getting lazier over time. Maybe the explanation is a lot simpler and it has to do with these reported usability issues that come with scale. Reliability in LLMs is a difficult research topic but a very important one. Paper: https://lnkd.in/eUMY_fY2â†“Join 85K+ AI researchers and devs so you donâ€™t miss my weekly summary of the top AI and LLM papers: https://lnkd.in/e6ajg945â€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_12": {"text": "Molmo and Pixo: Open Weight and Data for SoTA Multimodal ModelsPresents a new family of vision language models that are state-of-the-art in their class of openness. The best-in-class 72B model within the Molmo family not only outperforms others in the class of open weight and data models but also compares favorably against proprietary systems like GPT-4o, Claude 3.5, and Gemini 1.5 on both academic benchmarks and human evaluation.https://lnkd.in/d_iNRdpPâ†“Join 85K+ AI researchers and devs so you donâ€™t miss my weekly summary of the top AI and LLM papers: https://lnkd.in/e6ajg945â€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_13": {"text": "Llama 3.2 feels like a game-changer!From my few preliminary tests, the 11B instruct vision model is very capable. It's going to be interesting how developers use these lightweight models which are insanely fast. I am going to be developing mobile apps with these.With Llama now offering standard-size models, lightweight models, and vision models, the next move is a very capable reasoning model to match or come close to what o1 can do. The tooling and availability of all these different kinds of models are propelling us very fast into a new age of AI. By the way, you can test these models very easily on Fireworks AI. Full Llama 3.2 overview here: https://lnkd.in/d5TcMeSJâ€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_14": {"text": "AI voice agents are rapidly improving. I just had an interesting conversation with the Moshi voice agent. It's not perfect. It is a bit abrupt, interrupts a lot, and ignores some questions in the conversation. I almost lost it in this short conversation I had with it. ðŸ˜… https://lnkd.in/eHmaKgZdIt's exciting to see the artifacts finally open-sourced. I think architecture-wise there is probably a lot more innovation to come. But the key, as with LLMs, might be in the data quality.â€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_15": {"text": "A Comprehensive Evaluation of Quantized Instruction-Tuned LLMsThis work evaluates the performance of instruction-tuned LLMs across various quantization methods on models ranging from 7B to 405B.The key findings (from the paper):- quantizing a larger LLM to a similar size as a smaller FP16 LLM generally performs better across most benchmarks- performance varies significantly with different quantization methods, model size, and bit-width, with weight-only methods often yielding better results in larger models- task difficulty does not significantly impact accuracy degradation due to quantizationPaper: https://lnkd.in/eNdgnPrdâ†“Join 85K+ AI researchers and devs so you donâ€™t miss my weekly summary of the top AI and LLM papers: https://lnkd.in/e6ajg945â€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_16": {"text": "Here is a good study list if you want a good foundation for working and building with LLMs.I've spent the last couple of months turning these topics into self-paced courses that teach you the concepts and how to apply them to a variety of use cases. More in our new AI course catalog: https://lnkd.in/embaNV_dâ€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_17": {"text": "Personalized AI research assistants are coming!NotebookLM can now generate realistic podcasts from AI papers. This is probably one of the most interesting applications of AI and LLMs I've seen recently.Just listen to the audio summary of the Meta Prompting paper generated by NotebookLM. (sound on)Full walkthrough here: https://lnkd.in/dgSv-GjUâ€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_18": {"text": "I feel like with Cursor I've unlocked a superpower.I do a lot of work involving code and knowledge so the combination of Cursor with Claude 3.5 Sonnet is unlike any other AI service I've used. I am not affiliated with Cursor or being paid to say this. You should at least give it a go for like a day or two. Start with some simple to build intuition and get familiar with the interface and features. Then gradually do more advanced things with it such as debugging and generating tests. What has worked great for me is to build small  apps of tasks I do every day like transcribing, writing technical posts, tutorial idea generation, and course plans.â€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_19": {"text": "After the AI Research Scientist paper, the big question was whether LLM-based agents can even generate novel research ideas. This work finds that LLM-generated research ideas are judged as more novel (p <0.05) than human expert ideas; however, they were rated slightly weaker in terms of flexibility. They also report that LLM agents lack diversity in the idea generation process and are not reliable evaluators.Paper: https://lnkd.in/e88Hyybrâ€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_20": {"text": "In-context learning (ICL) in LLMs, while powerful, is not fully understood.This new paper explicitly studies in-context learning in a regression problem and argues that ICL uses a combination of both learning from in-context examples and retrieving internal knowledge. They are able to investigate this using an evaluation framework to compare ICL mechanisms across LLMs, datasets, and prompt configurations. Reports that LLMs can learn from regression examples of realistic datasets in-context, extending previous work on synthetic data to more practical scenarios. They also offer insights for optimizing applications via prompt engineering techniques that leverage a better understanding of in-context learning mechanisms. Paper: https://lnkd.in/epb4G-4jâ†“Join 85K+ AI researchers and devs so you donâ€™t miss my weekly summary of the top AI and LLM papers: https://lnkd.in/e6ajg945â€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_21": {"text": "alphaXiv is like community notes for AI papers. It also looks like a useful resource for finding good papers, insights, and research ideas. Definitely worth checking out!Here is my 10 min overview of alphaXiv: https://lnkd.in/epnC7vDvWeb: https://alphaxiv.orgI spend a lot of time on arXiv and very often have questions about results and claims. alphaXiv appears to want to solve this by enabling discussions on papers.â€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_22": {"text": "The Top ML Papers of the Week (August 26 - September 1):- GameGen- ReMamba- Transfusion- AutoGen Studio- Text2SQL is Not Enough- Agentic RAG for Time Series Analysis...â€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_23": {"text": "This week in AI research: a game engine powered entirely by a neural model and deeper integration of AI agents. If you're looking for more weekend reads, here are a few notable AI papers:- GameGen: a game engine powered by a diffusion model that enables real-time interaction with complex environments over long trajectories. Uses a two-phase training process involving an RL agent to learn and a diffusion model to generate frames. It can interactively simulate DOOM over at 20 fps on a single TPU. https://lnkd.in/ewTD8Rsi- Agentic RAG for Time Series Analysis: proposes an agentic RAG framework for time series analysis. Uses a multi-agent architecture where an agent orchestrates specialized sub-agents to complete time-series tasks. The sub-agents leverage tuned small language models and can retrieve relevant prompts containing knowledge about historical patterns and trends. This helps to improve predictions on new data. https://lnkd.in/ekGsFhfe- Smaller, Weaker, Yet Better: finds that weaker + cheaper (WC) models can generate better synthetic data for fine-tuning models compared to data generated with stronger but more expensive models. Overall, results suggest that WC models may be a compute-optimal approach for training advanced LLM reasoners. https://lnkd.in/eAgaYEP2- ReMamba: investigates the long-context capabilities and efficiencies of Mamba models. The long-context deficiency issues are due to Mamba's RNN-like nature; It achieves this by condensing information via the following compression strategy: the top-k hidden states during the first forward pass and leverages Mambaâ€™s selective mechanism to incorporate them into the state space during the second forward pass. Achieved a 3.2 improvement over the baseline on LongBench and a 1.6 improvement on L-Eval. The strategy seems to also transfer to Mamba 2. https://lnkd.in/estg_m-7- Persuasion Games with LLMs - claims that a multi-agent framework can be used to improve the persuasive efficacy of LLMs. The primary agent engages in persuasive dialogue while auxiliary agents perform key tasks like response analysis and information retrieval. Finds that LLMs are capable of creating a perspective change in the users and persuading them to make a purchase decision. For instance, Sales agents can achieve a 71% positive shift in user perspectives. https://lnkd.in/e-nMX79râ†“There are a few more exciting papers that I will highlight tomorrow in the Top ML Papers of the Week: https://lnkd.in/e6ajg945â€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_24": {"text": "Microsoft Research introduces AutoGen Studio, a low-code interface for rapidly prototyping AI agents. It's built on top of the AutoGen framework and can also be used for debugging and evaluating multi-agent workflows.https://lnkd.in/eDqSgzv9â€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_25": {"text": "Agentic RAG for Time Series AnalysisProposes an agentic RAG framework for time series analysis. Uses a multi-agent architecture where an agent orchestrates specialized sub-agents to complete time-series tasks.The sub-agents leverage tuned small language models and can retrieve relevant prompts containing knowledge about historical patterns and trends. This helps to improve predictions on new data.Extensive empirical studies demonstrate that the Agentic-RAG framework achieves performance on par with, or even surpassing, state-of-the-art methods across multiple time series analysis tasks for both univariate and multivariate datasets. The multi-agent approach tackles the diverse and complex challenges of time series analysis, unlike a single, universal agent that attempts to be a jack-of-all-trades for all time series tasks.https://lnkd.in/ekGsFhfeâ†“Join 80K+ AI researchers and devs so you donâ€™t miss my weekly summary of the top AI and LLM papers: https://lnkd.in/e6ajg945â€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_26": {"text": "Interesting new paper on automating the process of building powerful AI agents. Presents Meta Agent Search a meta agent that iteratively programs and tests new agents based on a growing archive of previous discoveries. Claims that with their approach it is possible to learn any possible agentic system including prompts, tool use, control flows, and more.They achieve this by focusing on three main components referred to as search space (define agents), search algorithm (explore search space), and the evaluation function (evaluate candidate agents).We consistently observe the surprising result that agents invented by Meta Agent Search maintain superior performance even when transferred across domains and models, demonstrating their robustness and generality.https://lnkd.in/ep7u_WsJâ†“Join 80K+ AI researchers and devs so you donâ€™t miss my weekly summary of the top AI and LLM papers: https://lnkd.in/e6ajg945â€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_27": {"text": "Enhancing Greedy Decoding with LLMs using Diverse ExemplarsUses a hybrid self-ensembling approach (based on diverse exemplars) to improve the overall performance of LLMS.Specifically, it uses diverse exemplars to generate multiple candidate responses and then aggregates them using an LLM to generate a final response. This approach achieves better accuracy compared to greedy decoding and lower cost compared to self-consistency approaches. https://lnkd.in/eFNng6u4â†“Join 80K+ AI researchers and devs so you donâ€™t miss my weekly summary of the top AI and LLM papers: https://lnkd.in/e6ajg945â€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_28": {"text": "The Top ML Papers of the Week (August 12 - 18):- rStar- HybirdRAG- LongWriter- EfficientRAG- RAGChecker- The AI Scientist...â€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_29": {"text": "RAGChecker: a fine-grained evaluation framework for diagnosing retrieval and generation modules in RAG.Shows that RAGChecker has better correlations with human judgment.Reports several revealing insightful patterns and trade-offs in design choices of RAG architectures.https://lnkd.in/eNfyV3s8â†“Join 80K+ AI researchers and devs so you donâ€™t miss my weekly summary of the top AI and LLM papers: https://lnkd.in/e6ajg945â€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_30": {"text": "Enhancing LLMs for RAGIntroduces RAGFoundry, an open-source framework for augmented LLMs for RAG use cases. It supports data creation, training, inference, and evaluation.One useful application is the creation of data-augmented datasets for tuning and evaluating  LLMs in RAG settings. After checking out their GitHub repo, I like how easy they make it to build and test quick prototypes with several RAG configurations and enhancements. It has a lot of potential even as just a learning tool.https://lnkd.in/eGu-8zpAâ†“Read by 75K+ AI researchers and devs, check out my weekly summary of the top AI and LLM papers. https://lnkd.in/e6ajg945â€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_31": {"text": "Structured output is all you need!Structured outputs can help to improve the performance and reliability of your LLM application. OpenAI just announced that they are now supporting structured outputs in the API. You can use it with function calling and defining your own JSON schema. Here is the FULL video if you are interested: https://lnkd.in/eh3BUEMSThe idea is to constrain the outputs based on defined schemas. There are many tools that can perform this for you such as instructor, outlines, and guidance. Very excited to use this in many of our use cases. I am working on a part 2 tutorial to dive deep into this feature and apply it to other common and creative use cases.â€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_32": {"text": "Excited that our course is in the Top 10 Maven courses this week.If you are interested in learning the most effective and efficient ways to prompt LLMs, this course is for you. Check it out here: https://lnkd.in/efXjpBzvWe teach a systematic approach to design and iterate on prompts, how to evaluate, and how to build complex workflows with LLMs. We believe prompt engineering is and will continue to be an important set of skills for AI engineers and AI product builders. Prompt engineering also helps you think more deeply about how to build with LLMs, improve LLM outputs, responsible and robust design, and how to combine creative LLM-powered components to build complex workflows with LLMs. After completing the training, you will have a good understanding of what prompting techniques and best practices to apply when building your LLM applications. You will have knowledge on the latest prompt engineering best practices, including how to leverage them in improving RAG and agentic systems.Please note that this course is not for everyone. It touches on advanced concepts meant for professionals (e.g., software engineers, full-stack devs, and technical product managers) interested in building real-world products and applications with LLMs.DM me if you have any questions about it.â€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_33": {"text": "Meta presents Self-Taught Evaluators, an approach to improve model-based evaluators using synthetic training data only. It first generates contrasting outputs (good and bad model responses) and trains an LLM-as-a-Judge to produce reasoning traces and final judgments.The self-improvement scheme repeats the training process in an iterative way using its improved predictions. Keep in mind that this doesn't use any labeled preference data so no human preference judgements are required. They claim to outperform LLM-judges such as GPT-4 and match top-performing reward models trained on labeled examples. Self-Taught Evaluator can improve a strong LLM (Llama3-70BInstruct) from 75.4 to 88.3 (88.7 with majority vote) on RewardBench.This is another interesting way to use synthetic data to iteratively improve the evaluation capabilities of the LLM. This sounds like a good application for small language models but I read in the paper that these were not tried. The seed model needs to have the capability to generate reasonable evaluations (i.e., already instruction-tuned to human preferences).https://lnkd.in/et2dV-eiâ†“Follow my weekly summary of the top AI and LLM papers. Read by 75K+ AI researchers and developers: https://lnkd.in/e6ajg945â€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_34": {"text": "The Top ML Papers of the Week (July 22 - July 28):- LazyLLM- Llama 3.1- OpenDevin- AlphaProof- Text-to-SQL Survey-Â RAG vs. Long-Context LLMs ...â€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_35": {"text": "After reading several AI papers this week, I observed lots of emphasis on complex math reasoning, efficient inference, efficient learning, and self-improving agents.If you are looking for some weekend reads, here are a few notable AI papers I read this week:- Llama 3.1: a collection of LLMs that include 8B, 70B, and 405B parameters models. Supports eight languages and extends the context window to 128K tokens; performs competitively and in some cases outperforms state-of-the-art models across capabilities like general knowledge, math reasoning, and tool use. https://lnkd.in/ex4FUcdz- RAG vs. Long-Context LLMs: compares RAG and long-context LLMs and finds that long-context LLMs outperform RAG on average performance while RAG is significantly less expensive. Proposes Self-Route, leveraging self-reflection to route queries to RAG or LC; reports that Self-Route significantly reduces computational cost while maintaining comparable performance to LC. https://lnkd.in/dznsRt95- LazyLLM: introduces a novel dynamic token pruning method for efficient long-context LLM inference. It can accelerate the prefilling stage of a Llama 2 7B model by 2.34x and maintain high accuracy. It selectively computes the KV for tokens that are important for the next token prediction in both the prefilling and decoding stages. LazyLLM allows language models to dynamically select different subsets of tokens from the context in different generation steps, even though they might be pruned in previous steps. https://lnkd.in/e9DDZc4w- Teaching LLM Agents to Self-Improve: claims it is possible to iteratively fine-tune LLMs with the ability to improve their own response over multiple turns with additional environment feedback. The LLM learns to recursively detect and correct its previous mistakes in subsequent iterations. Improves the self-improvement abilities of 7B models on reasoning tasks (GSM8K and MATH), attaining an improvement over turns that are unseen in strong proprietary models. https://lnkd.in/eyUnSc9E- Model Collapse on Synthetic Data: investigates the effects of training models on recursively generated data. Finds that training on model-generated content can cause irreversible defects where the original content distribution disappears. This shows that the effect, referred to as model collapse, occurs in LLMs, VAEs, and GMMs; while tested on smaller scale models (~100M params), the authors suggest this effect is highly likely to transfer to larger models over time. https://lnkd.in/eSdZGHefThere are a few more exciting papers that I will highlight tomorrow in the Top ML Papers of the Week: https://lnkd.in/e6ajg945â€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_36": {"text": "Excited to see that Mistral Large 2 is competitive with Llama 3.1 405B on several benchmarks.I noticed a few interesting details in the Mistral Large 2 announcement like concise outputs, steerability, instruction following, more language support, and reduced hallucination.Here is my overview and tests of Mistral Large 2: https://lnkd.in/e_ZaXtVUBigger, better, and more open models is good for all.â€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_37": {"text": "You should check out OpenDevin if you are learning about or building AI agents today.The team has published now a technical report on it. OpenDevin is a platform to develop generalist agents that interact with the world through software. Features include: - an interaction mechanism for interaction between agents, interfaces, and environments- environment: sandboxed operating system + web browser available to the agents- interface to create and execute code- multi-agent support- evaluation framework --10 implemented agentsMIT License28K GitHub stars160 contributors1.3K contributionshttps://lnkd.in/ekgxbeYhâ†“Follow my weekly summary of the top AI and LLM papers. Read by 70K+ AI researchers and developers: https://lnkd.in/e6ajg945â€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_38": {"text": "A Survey of Prompt Engineering Methods in LLMsHuge collection of prompt engineering methods for all sorts of NLP tasks.https://lnkd.in/eDvrRESFâ†“Follow my weekly summary of the top AI and LLM papers. Read by 65K+ AI researchers and developers: https://lnkd.in/e6ajg945â€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_39": {"text": "That's right! It's a huge week for small language models (SLMs)Few new SLMs on my radar:1) Mistral NeMoHighlights: - Introduced by Mistral + NVIDIA- Apache 2.0 license- outperforms Gemma 2 9B and Llama 3 8B- multilingual capabilities- efficient tokenizer (Tekken)2) GPT-4o miniHighlight: 15 cents per million input tokens, 60 cents per million output tokens, MMLU of 82%, and fast.3) SmolLMHighlight: SmolLM models: 135M, 360M, and 1.7B parameters; Smollm-Corpus curated from Cosmopedia v2, FineWeb-Edu, and Stack-Edu-Python.4) Mathstral and Codestral MambaHighlights:- Mathstral achieves 56.6% on MATH and 63.47% on MMLU.- Codestral Mamba tested on in-context retrieval capabilities up to 256k tokens and shown to be quite efficient due to Mamba architecture.5) H2O Danube3Highlight: After final tuning, they show strong performance on academic, chat, and fine-tuning benchmarks. H2O-Danube3 is efficient enough to run on modern smartphones, allowing local and fast processing on mobile devices.Will be doing a YT video on this including capabilities and interesting ways to apply SLMs. Stay tuned! https://lnkd.in/e2WS8ksJAny others?â€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_40": {"text": "Solid report sharing practical tips for developing with and evaluating LLMs.Solutions covered range from ReAct to RAG to parameter-efficient methods. It's almost like a book compressed into a short tutorial. Great way to catch up with the field.It also includes a GitHub Repo with slides and code.https://lnkd.in/eSMw8xbpâ†“Follow my weekly summary of the top AI and LLM papers. Read by 65K+ AI researchers and developers: https://lnkd.in/e6ajg945â€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_41": {"text": "Can LLMs Do Retrieval and Reasoning in 1 Million Context Window?Proposes a framework (NeedleBench) of progressively challenging tasks to assess the long-context retrieval and reasoning capabilities of LLMs.They also present the Ancestral Trace Challenge that increases the need for complex logical reasoning which is common in real-world long-context tasks. (see the performance decline trend for popular models in the figure)Their findings suggest that current LLMs struggle to handle reasoning tasks with complex logical relationships, even with texts shorter than 2K tokens.The big question for me has always been whether LLMs can retrieve and integrate multiple pieces of dispersed information. This is very common in real-world tasks and is challenging to get right. This NeedleBench test looks like a reasonable next step for the Needle In A Haystack type of task.https://lnkd.in/dDvm33UMâ†“Follow my weekly summary of the top AI and LLM papers. Read by 65K+ AI researchers and developers: https://lnkd.in/e6ajg945â€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_42": {"text": "This AI-powered Juypter implementation has huge potential.They have added Mistral's Codestral and GPT4-o for inline AI Copilot auto-complete, code generation, editing, error fixing, and sidebar chat. I like that it feels more like a collaborative tool and less like a tool to replace the overall experience of coding. GitHub: https://lnkd.in/eywnKiZgMy tutorial: https://lnkd.in/eWdvbeEWâ†“Follow my weekly summary of the top AI and LLM papers. Read by 65K+ AI researchers and developers: https://lnkd.in/e6ajg945â€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_43": {"text": "// Searching for Best Practices in RAG //Cool paper showing the best practices for building effective RAG workflows. Proposes strategies that focus on performance and efficiency, including emerging multimodal retrieval techniques. https://lnkd.in/ehDD46N3â†“Follow my weekly summary of the top AI and LLM papers. Read by 65K+ AI researchers and developers: https://lnkd.in/e6ajg945â€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_44": {"text": "This is one of the coolest ideas for scaling synthetic data that I've come across.Proposes 1 billion diverse personas to facilitate the creation of diverse synthetic data for different scenarios.It's easy to generate synthetic data but hard to scale up its diversity which is essential for its application.This paper proposes a novel persona-driven data synthesis methodology to generate diverse and distinct data covering a wide range of perspectives. Previous works synthesize data using either instance-driven approaches (e.g., using seed corpus) or key-point-driven methods (e.g., using topic/subject). Both of these approaches lack the desired coverage, quality, and perspectives needed to robustly scale the data synthesis process.To measure the quality of the synthetic datasets, they performed an out-of-distribution evaluation on MATH. A fine-tuned model on their synthesized 1.07M math problems achieves 64.9% on MATH, matching the performance of gpt-4-turbo-preview at only a 7B scale.Their method is not only effective for MATH problems, but it can also be used to generate logical reasoning problems, instructions, game NPCs, tool development, knowledge-rich text, and many more use cases. https://lnkd.in/eYMpHMqsâ†“Follow my weekly summary of the top AI and LLM papers. Read by 65K+ AI researchers and developers: https://lnkd.in/e6ajg945â€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_45": {"text": "Lots of exciting developments in AI and LLMs in the past couple of days. 1) Heavy Use of Synthetic DataFor instance, I am noticing a lot of applications of synthetic data for all sorts of tasks ranging from unlocking reasoning capabilities in LLMs to training large-scale biological models. 2) Leveraging Structured Inputs and OutputsOne trend in LLM research that has been fascinating is the leveraging of external structures like trees and graphs to enhance LLMs. What we are seeing is that it's not only useful to structure outputs to mitigate erroneous outputs but also to structure inputs via complex structures to enable better task understanding. 3) Hybrid LLMsIt's also interesting to see ideas that propose hybrid systems to make the most out of LLMs. An interesting new paper proposed LongRAG where they combined long-context LLMs with RAG to improve the overall system by significantly lowering the burden of retrievers.The best way to help you keep track of all these developments is via my LLM News series. Here is my new episode:â€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_46": {"text": "The context caching feature for Gemini is really neat!As you all know, I write a lot and document AI research progress in lots of places. But I also tend to forget research findings due to the vast amount of papers I read so I wanted to try whether context caching can help me with some analysis and queries on papers I've summarized in the past. The idea is to cache all the papers I've summarized + system instructions (this can get very detailed) and then run complex queries on top of it using Gemini-1.5 Flash.Here is a quick demo of the idea with a code example â†“https://lnkd.in/eVMhBcZTâ€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_47": {"text": "PlanRAGEnhances decision making with a new RAG technique called iterative plan-then-RAG (PlanRAG).Step 1: an LM generates the plan for decision making by examining data schema and questionsStep 2: the retriever generates the queries for data analysisThe final step checks if a new plan for further analysis is needed and iterates on previous steps or makes a decision on the data. PlanRAG is more effective than iterative RAG on the proposed Decision QA tasks. Cool technique and it is not surprising that iterative planning could enhance decision-making capabilities of RAG systems. This follows the current theme of enriching RAG with reasoning modules, in this case, iterative planning.https://lnkd.in/eD9siYGNâ†“Tracked by 65K+ researchers and developers already, you can also follow my weekly summary of the top AI and LLM papers here: https://lnkd.in/e6ajg945â€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_48": {"text": "From RAG to Rich ParametersInvestigates more closely how LLMs utilize external knowledge over parametric information for factual queries.Finds that in a RAG pipeline, LLMs take a â€œshortcutâ€ and display a strong bias towards utilizing only the context information to answer the question, while relying minimally on their parametric memory.Quote: Through attention contributions, attention knockouts and causal traces, we specifically observe a reduced reliance on the subject token, and the MLP activations associated with it, when the context is augmented with RAG.https://lnkd.in/egYfUMTgâ†“Tracked by 65K+ researchers and developers already, you can also follow my weekly summary of the top AI and LLM papers here: https://lnkd.in/e6ajg945â€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_49": {"text": "Best open-source video generation model?It probably is and itâ€™s not even close.Iâ€™ve been closely tracking the Open-Sora project, from HPC-AI Tech, and their new release is impressive.You can now generate 16-second 720p videos with Open-Sora!Release includes:> 1.1B model on >30 m data> generates up to 16-second videos> 144p to 720p supported> image-to-video now supported> enhanced diffusion model> video compression network (VAE) for spatial and temporal compression> reduces training costs and ensures video smoothness> increased controllabilityBest part?The model weights and training code have been fully open-sourced.Kind of amazing how many developments we have had in just the past few weeks around video generation. After OpenAI's Sora, notable examples include KLING, Dream Machine, and Gen-3 Alpha.GitHub: https://lnkd.in/et2kN7ibExample of generated clips:â€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_50": {"text": "Thought-Augmented Reasoning with LLMsPresents a thought-augmented reasoning approach, Buffer of Thoughts, to enhance the accuracy, efficiency, and robustness of LLM-based reasoning. It leverages a meta-buffer containing high-level thoughts (thought templates) distilled from problem-solving processes.The relevant thought template is then retrieved and instantiated with task specific reasoning structures for the thought-augmented reasoning process.They also propose a buffer manager to distill thought templates from various solutions and dynamically update the meta buffer. This ensures the scalability, stability, and capacity of the meta buffer as more tasks are solved.It demonstrates SOTA performance on 10 challenging tasks while requiring 12% of the cost of multi-query prompting methods like Tree-of-Thoughts. The main idea here is to improve the generalization of the reasoning process by maintaining high-level thoughts obtained through experience and used for guidance.(link in the comments)â€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_51": {"text": "MatMul-free LLMsProposes an implementation that eliminates matrix multiplication operations from LLMs while maintaining performance at billion-parameter scales. The performance between full precision Transformers and the MatMul-free models narrows as the model size increases.Quote from the paper: By utilizing an optimized kernel during inference, our modelâ€™s memory consumption can be reduced by more than 10Ã— compared to unoptimized models. There is a huge potential to use this type of approach to significantly reduce both memory usage and latency in LLMs. It's much needed especially for building agentic workflows and other types of advanced LLM applications that require better memory usage and latency.Lots of interesting results in this paper and I am now curious what a 100B+ parameter MatMul free model or even a long-context LLM based on this approach can do. Efficiency is a huge constraint for LLMs so it's always exciting to see works that optimize core Transformer operations and overall architecture.(link in the comments)â€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_52": {"text": "The Geometry of Concepts in LLMsStudies the geometry of categorical concepts and how the hierarchical relations between them are encoded in LLMs. Finding from the paper: Simple categorical concepts are represented as simplices, hierarchically related concepts are orthogonal in a sense we make precise, and (in consequence) complex concepts are represented as polytopes constructed from direct sums of simplices, reflecting the hierarchical structure.(link in the comments)â€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_53": {"text": "Teaching LLMs to Express ConfidencePresents SaySelf, a training framework to teach LLMs to express more accurate fine-grained confidence estimates and self-reflective rationales. It performs supervised finetuning on a dataset that contains summaries of the difference between multiple reasoning chains.Reinforcement learning is then applied to calibrate confidence estimates, encouraging the LLM to produce accurate, high-confidence predictions and penalize overconfidence in erroneous outputs.Quote from the paper: SaySelf reduces calibration errors, maintains performance, and generates insightful rationales.(paper link in the comments)â€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_54": {"text": "The Top ML Papers of the Week (May 13 - May 19):- Veo- GPT-4o- Chameleon- RLHF Workflow- Gemini 1.5 Flash- Fine-tuning and Hallucinations...(click the article for all paper summaries and links)â€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_55": {"text": "Big week in the world of AI and LLMs.OpenAI did amazing today.Now looking forward to all the big AI news at Google I/O.Letâ€™s connect tomorrow if you are around.By the way, I am far from being an actual influencer ðŸ˜Š. But we are building cool stuff with LLMs DAIR.AI.â€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_56": {"text": "Nice addition to the OpenAI Playground!Side-by-side comparison of models is now possible. I constantly iterate my prompts on the Playground and this has been one of my biggest asks. Thank you!â€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_57": {"text": "Great demo by the OpenAI team!Some quick thoughts:It's really impressive how smooth and personalized collaboration with AI is getting. I particularly enjoyed the math and storytelling examples. Not sure if you noticed but I think this is the first time an AI system almost delivered the entire tech demo itself. Okay, not the entire thing but a huge part of it. This points to a future where humans are required to do less and communicate more efficiently with AI systems to be more productive. That's probably what AI assistants are meant to do. Lots of implications there. I disliked the coding demo. I am not sure if actual developers program that way. It doesn't feel like an intuitive interface. This tells me that AI with advanced multimodal reasoning abilities is going to advance some domains more than others. I still prefer to use Copilot as my coding assistant. The translation demo was cool but I have seen Mark Zuckerberg deliver a better AI-powered translation demo a couple of years back.After the demo, it's very clear to me that advanced emotional intelligence, something I researched heavily during my PhD, is the key to unlocking the future of collaborative AI (AGI or whatever they are going to be calling it). OpenAI is taking the right steps towards that.GPT-4o looks like a very strong model with reasoning capabilities. So excited that it's going to be available via the APIs. Will be testing that a lot.Also, the reasoning performance (check the bar chart below) of these models tells a remarkable story of the progress many LLM providers are making. The differences in performance have reduced significantly.I am now even more excited about what Llama 3 400b is going to bring.What are your takeaways?â€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_58": {"text": "A Survey on Retrieval-Augmented Language ModelsThis paper covers the most important recent developments in RAG and RAU systems. It includes evolution, taxonomy, and an analysis of applications. There is also a section on how to enhance different components of these systems and how to properly evaluate them. It concludes with a section on limitations and future directions.Paper: https://lnkd.in/eixnx6bF---Tracked by 55K+ researchers and developers already, you can also follow my weekly summary of the top AI and LLM papers here: https://lnkd.in/e6ajg945â€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_59": {"text": "When to Retrieve?This new paper presents an approach to train LLMs to effectively utilize information retrieval.It first proposes a training approach to teach an LLM to generate a special token, <RET>, when it's not confident or doesn't know the answer to a question.The fine-tuned model outperforms a base LLM  in two fixed alternate settings that include never retrieving and always retrieving context. We have seen other works showing that LLMs combined with retrievers can lead to unfaithful RAG systems. This work addresses the problem by fine-tuning the model to determine whether it needs additional context or not to answer questions. Training LLMs using special tokens continues to be an interesting approach not only for improving the accuracy of LLMs but also their efficiency.Paper: https://lnkd.in/e9YnV9Ez---Tracked by 55K+ researchers and developers already, don't miss out on my weekly summary of the top AI and LLM papers here: https://lnkd.in/e6ajg945â€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_60": {"text": "The Top ML Papers of the Week (April 22 - April 28):- Phi-3- OpenELM- AutoCrawler- Self-Evolution of LLMs- AI-powered Gene Editors- Make Your LLM Fully Utilize the Context...(click the article for the full list of papers and summaries)â€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_61": {"text": "Next week I will be delivering a FREE 30-minute lesson on building complex applications with LLMs. I will cover common design patterns and the necessary techniques to build effective LLM-based systems like agentic workflows and RAG. You don't need any programming knowledge as I will be using Flowise AI, a no/low-code tool for building complex LLM chat flows that combine LLMs with external tools and knowledge bases.It's never too late to get started with LLMs. This is a great opportunity to learn how to get started with LLMs or even expand beyond what you are already building.Sign up here: https://bit.ly/3xNeugaâ€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_62": {"text": "Reducing Hallucination in Structured Outputs via RAGNice paper by researchers at ServiceNow where they discuss how to deploy an efficient RAG system for structured output tasks.The RAG system combines a small language model with a very small retriever. It shows that RAG can enable deploying powerful LLM-powered systems in limited-resource settings while mitigating issues like hallucination and increasing the reliability of outputs.The paper covers the very useful enterprise application of translating natural language requirements to workflows (formatted in JSON). So much productivity can come from this task but there is a lot of optimization that can be further achieved (eg., using speculative decoding or using YAML instead of JSON).Nothing too special in the paper but there are some great insights and practical tips on how to effectively develop RAG systems for the real world. This is my favorite kind of AI report. Paper: https://lnkd.in/e3jFuf63---Enjoy AI papers? Join 50k+ researchers and developers for my weekly summary of AI and LLM papers here: https://lnkd.in/e6ajg945â€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_63": {"text": "The Top ML Papers of the Week (April 8 - April 14):- CodeGemma- Infini-Transformer- Overview of Multilingual LLMs- LM-Guided Chain-of-Thought- The Physics of Language Models- Best Practices and Lessons on Synthetic Data...(click the article for all paper summaries and links)â€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_64": {"text": "Best Practices and Lessons Learned on Synthetic Data for Language ModelsGreat overview by Google DeepMind on synthetic data research.It covers applications, challenges, and future directionsThis is an important paper given the significant advancements we are seeing from the use of synthetic data in the field of AI.We know for sure that the more high-quality data we give these models, the better the performance. Creating synthetic data is not hard but ensuring its quality is really the challenge.The paper also discusses important topics when working with synthetic data such as ensuring quality, factuality, fidelity, unbiasedness, trustworthiness, privacy, and more.There are a lot of great references mentioned in the related work section as well. A great weekend read if you are working or developing with LLMs or generative AI.Paper: https://lnkd.in/eH_ubBby---Love reading papers? Join 50k+ researchers and developers for my weekly summary of LLM papers here: https://lnkd.in/e6ajg945â€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_65": {"text": "Efficient Infinite Context TransformersVery exciting paper by Google that integrates compressive memory into a vanilla dot-product attention layer. The goal is to enable Transformer LLMs to effectively process infinitely long inputs with bounded memory footprint and computation.They propose a new attention technique called Infini-attention which incorporates a compressive memory module into a vanilla attention mechanism. It builds in both masked local attention and long-term linear attention into a single Transformer block. This allows the Infini-Transformer model to efficiently handle both long and short-range contextual dependencies. This approach outperforms baseline models on long-context language modeling with a 114x compression ratio of memory!They also show that a 1B LLM can naturally scale to a 1M sequence length and a 8B model achieves a new SoTA result on a 500K length book summarization task.Given how important long-context LLMs are becoming having an effective memory system could unlock powerful reasoning, planning, continual adaption, and capabilities not seen before in LLMs. Great paper!Paper: https://lnkd.in/eCi9r5YK---Join 50k+ researchers and developers for my weekly summary of LLM papers and research developments here: https://lnkd.in/e6ajg945â€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_66": {"text": "LLMs for University-Level Coding CourseThis study finds that the latest LLMs have not surpassed human proficiency in physics coding assignments.Also finds that GPT-4 significantly outperforms GPT-3.5 and prompt engineering can further enhance performance.Quote from the paper: Should the improvement trajectory from GPT-3.5 to GPT-4 continue, LLMs may soon outperform student capabilities. Additionally, our analysis revealed that plots generated by LLMs are distinguishable from student-created ones due to their often misaligned or skewed layouts and a tendency to utilize default color schemes.Simple coding tasks may be easy for LLMs today but it gets more challenging with domain-specific coding tasks like physics coding. If LLMs continue improving, it's expected to see progress in some of these domains but this work highlights the importance of integrating human supervision for evaluating the results of an LLM for these types of tasks.â€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_67": {"text": "Here is a great opportunity to learn about building complex AI Agents. You will learn how to adopt AI agents for advanced applications such as answering security questionnaires, creating legal documents, automating customer support, and creating dashboards.You can join the FREE 2-hour workshop (certificate included) by Abacus AI here: https://lnkd.in/e5UacRa9â€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_68": {"text": "314 Billion Parameter Grok-1 Inference Accelerated by 3.8xAfter the Grok-1 open release, the Colossal-AI team followed up immediately with an easy-to-use Python + PyTorch + HuggingFace version of Grok-1 for AI developers.This offering accelerates Grok-1 inference by 3.8x. On an 8xH800 80GB server, the inference latency is accelerated by nearly 4 times compared to methods such as JAX and HuggingFace's auto device map.Try it out here: https://lnkd.in/eectEMWQâ€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_69": {"text": "The Top ML Papers of the Week (March 18 - March 24):- Grok-1- TacticAI- Agent-FLAN- LLM4Decompile- Evolutionary Model Merge- Retrieval-Augmented Fine-Tuning...(all papers summaries and links in the article)â€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_70": {"text": "Can LLMs Reason and Plan?There is a lot of debate about whether LLMs can reason and plan. These are important capabilities for unlocking complex applications with LLMs such as in the domains of robotics and autonomous agents. This position paper discusses the topic of reasoning and planning for LLMs. Here is a summary of the author's conclusion: To summarize, nothing that I have read, verified, or done gives me any compelling reason to believe that LLMs do reasoning/planning, as normally understood. What they do instead, armed with web-scale training, is a form of universal approximate retrieval, which, as I have argued, can sometimes be mistaken for reasoning capabilitiesI sometimes refer to LLMs as retrieval on steroids so it's interesting to see it referred to as a form of universal approximate retrieval in this piece. I think we should have more work on theoretical analysis and rigorous evaluation to better understand whether reasoning and planning are truly possible with LLMs.What do you think?â€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_71": {"text": "Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision ModelsNot sure how I missed this report. It includes a review of Sora and some of the key developments powering this model, including limitations and opportunities of large vision models. Keep in mind that we don't have all the official details of Sora but I still appreciate the literature review and discussion.â€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_72": {"text": "RAG for AI-Generated ContentCool survey paper providing an overview of RAG used in different generation scenarios like code, image, audio,...I like the taxonomy of RAG enhancements and it seems to mention a lot of key RAG papers. Paper: https://lnkd.in/dNcYa3cdâ€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_73": {"text": "Knowledge-Augmented Planning for LLM AgentsProposes an approach to enhance the planning capabilities of LLMs through explicit action knowledge.It uses an action knowledge base and a knowledgeable self-learning phase to guide the model's action generation, mitigate planning hallucination, and enable continuous improvement. Outperforms existing baselines and shows the potential of integrating external action knowledge to streamline planning with LLMs and solve complex planning challenges.(paper in the comments)â€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_74": {"text": "Karpathy always delivers when it comes to AI education. As with any of his LLM lectures, I am familiar with the content but he is a great educator and always offers intuitive explanations of concepts. I highlight recommend checking his recent LLM lectures.His latest lecture is on building a GPT tokenizer. I teach a course on prompting techniques and improving the reliability of LLMs and we discuss many issues that stem from tokenization. This slide nicely summarizes a few of these weird behaviors of LLMs that we also observe in our use cases.Lecture: https://lnkd.in/ehmhqzWFâ€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_75": {"text": "The Gemini 1.5 Pro model guide is live! With support of up to 1 million tokens context length, you may be wondering what's possible with Gemini 1.5 Pro.My overall impression after our first round of testing is that Gemini 1.5 Pro is among the most powerful long context LLMs available today. I've published a summary of Gemini 1.5 Pro's capabilities along with concrete examples in the prompting guide. These are just preliminary tests. I will continue to analyze and document the model's capabilities and limitations. Stay tuned!From preliminary experiments, Gemini 1.5 Pro shows impressive capabilities around multimodal reasoning, video understanding, long document question answering, code reasoning on entire codebases, and in-context learning.One insight from testing this model is that we will have different kinds of LLMs that support different types of use cases. Gemini 1.5 Pro is not meant to be a model to reign among all. The long context LLMs are not meant to cover every use case imaginable, they are meant to unlock complex use cases that were unimaginable before with LLMs. Link to the guide in the comments â†“â€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_76": {"text": "The inference speed on the Groq examples didn't look real. So I tested it myself and I don't even know what to say about this. Need to take a closer look at the technical papers. For now, all I can think about is the complex use cases this, and the support of millions of tokens context length, can enable. With breakthroughs in inference and long context understanding, we are officially entering a new era in LLMs.I am not surprised that we now have a dedicated inference engine for language processing. From the groq FAQ:An LPU has greater compute capacity than a GPU and CPU in regards to LLMs. This reduces the amount of time per word calculated, allowing sequences of text to be generated much faster. Additionally, eliminating external memory bottlenecks enables the LPU Inference Engine to deliver orders of magnitude better performance on LLMs compared to GPUs.Try it yourself: https://groq.com/What you see in the clip is playing at its original speed. This also made me realize how slow I type.â€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_77": {"text": "Recurrent Memory Finds What LLMs MissExplores the capability of transformer-based models in extremely long context processing. Finds that both GPT-4 and RAG performance heavily rely on the first 25% of the input, which means there is room for improved context processing mechanisms.The paper reports that recurrent memory augmentation of transformer models achieves superior performance on documents of up to 10 million tokens. The recurrent memory seems to enable effective multi-hop reasoning which is challenging for current LLMs and RAG systems. It also has the desirable effect of filtering out irrelevant information which is key in long context processing.With all the recent releases of long context models, this is an interesting and timely paper. I like the idea of combining both the recurrent memory and retrieval to these large models to make them more generalizable for complex tasks that require long context processing.https://lnkd.in/ehC6muuKâ€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_78": {"text": "Indirect Reasoning with LLMsChain-of-thought (CoT) prompting and Self-Consistency follow direct reasoning frameworks to improve the performance of LLMs. While powerful in eliciting reasoning in LLMs, these techniques are often not enough for real-world tasks. To address this, a new paper proposes an indirect reasoning method to strengthen the reasoning power of LLMs. It employs the logic of contrapositives and contradictions to tackle IR tasks such as factual reasoning and mathematic proof.It consists of two key steps:- enhance the comprehensibility of LLMs by augmenting data and rules (i.e., logical equivalence of contrapositive)- design prompt templates to stimulate LLMs to implement indirect reasoning based on proof by contradictionExperiments on LLMs like GPT-3.5-turbo and Gemini-pro show that the proposed method enhances the overall accuracy of factual reasoning by 27.33% and mathematic proof by 31.43% compared to traditional direct reasoning methods.It's possible to also combine the indirect and direct reasoning methods to further boost the reasoning abilities of LLMs.This is fascinating and now makes me think about other potential complex logical laws we could continue exploring and integrating to further improve reasoning in LLMs.(paper in the comments)â€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_79": {"text": "LLM Agent for Large-Scale API CallsCool research paper presenting AnyTool, an LLM-based agent that can utilize 16000+ APIs from Rapid API. Proposes a simple framework consisting of - a hierarchical API-retriever to identify relevant API candidates to a query- a solver to resolve user queries- and a self-reflection mechanism to reactivate AnyTool if the initial solution is impracticable This tool leverages the function calling capability of GPT-4 so no further training is needed. The hierarchical API-retriever is inspired by a divide-and-conquer approach to help reduce the search scope of the agents which leads to overcoming limitations around context length in LLMs. The self-reflection component helps with resolving easy and complex queries efficiently.Experiments demonstrate strong performance from AnyTool that outperforms other baselines like ToolLLM and a GPT-4 variant tailored for tool utilization. A nice benchmark has also been proposed to better reflect practical application scenarios. Two future research directions are also proposed in the paper: 1) optimizing the organization of APIs for improving performance and efficiency.2) developing open-source LLMs specifically for API utilization.A great read for LLM practitioners and researchers. (paper in the comments)â€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_80": {"text": "RAG for LLMsBeen doing some deeper exploration into RAG and the ecosystem.I believe a strong starting point is the survey of Gao et al.: Retrieval-Augmented Generation for Large Language Models: A Survey.I liked the paper so much that I wrote a shorter summary of it to highlight the key points, insights, and practical tips about building RAG systems. Will also be adding an easy-to-follow bibliography to help track new developments in RAG research. I am also working on technical coding tutorials for this guide to show how to apply some of the strategies to improve RAG systems. RAG has become one of the popular ways to build with LLMs. With these guides, I hope to make the research ideas more accessible. Summary of RAG for LLMs: https://lnkd.in/e39fhreeâ€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_81": {"text": "LLM based Multi-AgentsDiscusses the essential aspects of LLM-based multi-agent systems.It includes a summary of recent applications for problem-solving and word simulation.It also discusses datasets, benchmarks, challenges, and future opportunities to encourage further research and development from researchers and practitioners.â€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_82": {"text": "Red Teaming Visual Language ModelsNice paper on red teaming vision-language models. It first presents a red teaming dataset of 10 subtasks (e.g., image misleading, multi-modal jailbreaking, face fairness, etc). Finds that 10 prominent open-sourced VLMs struggle with the red teaming in different degrees and have up to 31% performance gap with GPT-4V.They also apply red teaming alignment to LLaVA-v1.5 with SFT using the proposed red teaming dataset, which improves model performance by 10% in the test set.It's good to see these efforts ahead of the massive wave of vision language models that are on the horizon. Don't remember that we had such efforts with LLMs in the initial explosion of these systems. Proactive efforts are important in AI so I think this work is important to understand better the limitations and issues with these powerful systems.Paper: https://lnkd.in/eqjv-c7S---I also break down the latest LLM research papers every week: https://lnkd.in/eUxHMAaPâ€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_83": {"text": "Amazing! The short LLMOps course is now available on Udacity.Super excited for what the community builds from the lessons in the course.Also, huge shoutout to Comet and their incredible team for all the dedication towards open AI education.â€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_84": {"text": "ML YouTube Courses (~12Kâ­ï¸)It's impressive how many high-quality free AI courses there are.To make them easy to find, I maintain this collection with some of my favorite courses around topics like NLP, deep learning, LLMOps, maths for ML, and more. This has been useful for thousands of students, researchers, and even developers around the world.https://lnkd.in/dFdgnBbdâ€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_85": {"text": "LLM CourseI just came across this beautiful and comprehensive resource on LLMs. It includes notebooks, key references, and roadmaps.There is something to learn for everyone. For students, researchers, and practitioners.The Prompt Engineering Guide is also referenced, which is cool to see. One observation as I was reviewing the references is how much hard work the ML community dedicates toward open and high-quality education. This resource does a great job of organizing all those incredible LLM educational resources that exist out there. One topic I would add is LLMOps. But to be fair, the majority of the topics are roughly covered in the LLM Engineer Roadmap.Highly recommended! And last but not least, many thanks to Maxime Labonne for releasing this excellent resource. LLM Course: https://lnkd.in/g3F6UrUdâ€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_86": {"text": "There is something so special and empowering about running LLMs on your local machine.Keep in mind that these things are still pretty much in their raw state. Not hard to visualize an LLM-powered operating system at some point.The interactions just feel natural (and this is because I am running language models with the terminal). No UI. I find that I am also more patient with my imperfect local language model than I am with other advanced LLMs and advanced visual playgrounds. Not sure what the psychology is there, maybe it's a case of proximity, but it's just a different vibe.This is why I am excited about small language models and even faster inference.â€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_87": {"text": "Blending Is All You NeedBased on the last month of LLM research papers, it's obvious to me that we are on the verge of seeing some incredible innovation around small language models.Llama 7B and Mistral 7B made it clear to me that we can get more out of these small language models on tasks like coding and common sense reasoning.Phi-2 (2.7B) made it even more clear that you can push these smaller models further with curated high-quality data. What's next? More curated and synthetic data?  Innovation around Mixture of Experts and improved architectures? Combining models? Better post-training approaches? Better prompt engineering techniques? Better model augmentation? I mean, there is just a ton to explore here as demonstrated in this new paper that integrates models of moderate size (6B/13B) which can compete or surpass ChatGPT performance. https://lnkd.in/gK2QuS3Tâ€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_88": {"text": "From LLM to Conversational AgentProposes RAISE, an advanced architecture to enhance LLMs for conversational agents. It's inspired by the ReAct framework and integrates a dual-component memory system. It utilizes a scratchpad and retrieved examples to augment the agent's capabilities. The scratchpad serves as a transient storage (akin to short-term memory) and the retrieval module operates as the agent's long-term memory. So you can think of this as a framework that combines ReAcT, a scratchpad, and a retrieval system.This system mirrors human short-term and long-term memory and helps to maintain context and continuity which are key in conversational systems.One benefit of such a system is the ability to customize and control the behavior of the conversational system. This work also shows the potential to fine-tune the LLM within the RAISE framework which leads to enhanced controllability. RAISE was tested on the real-estate domain and showed superior performance as compared to the conventional conversational agents. For every use case, there might be a need to control for different aspects of an LLM-powered agent. This can range from customizing the working memory components to changing the effectiveness of retrieval of certain types of information.I like the idea of modularity in a conversational agent but that means there are more variables to control. Lots of potential with this framework for building more tailored and personalized agents. (paper link in the comments)â€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_89": {"text": "The Top ML Papers of the Week (Jan 1 - Jan 7):- DocLLM- Mobile ALOHA- Self-Play Fine-tuning- Fast Inference of MoE- LLM Augmented LLMs- Mitigating Hallucination in LLMs...(all paper summaries and links in the article)â€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_90": {"text": "Unified Library for Evaluating LLMsNeat project by Microsoft offering a unified library to evaluate LLMs. A lot of the effort when working with LLMs goes into building a robust evaluation pipeline which could involve different models, tasks, and prompts. PrompBench looks like a promising library to support comprehensive evaluation and analysis of LLMs. It consists of functionalities for prompt construction, prompt engineering, dataset and model loading, adversarial prompt attack, dynamic evaluation protocols, and analysis tools.paper: https://lnkd.in/eEz3HTXScode: https://lnkd.in/etgzH9vb--If interested, I also provide technical summaries of the latest LLM research here: https://lnkd.in/embzyF3Fâ€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_91": {"text": "The Top ML Papers of the Week (Dec 18 - Dec 24): - VideoPoet- PowerInfer- LLM in a Flash- ReST Meets ReAct- Survey of RAG for LLMs- Geminiâ€™s Language Abilities...(all paper summaries and links in the article)â€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_92": {"text": "LLM Interpretability hasn't received as much attention but it's as important as any other research area to keep advancing LLMs. This area of study has implications for how to optimize and make LLMs more performant, reliable, and safer. I really like this repo that curates tools and recent papers on LLM interpretability.Great way to catch up on an exciting and important topic. Regardless if you follow the topic or not, the repository contains a good set of papers worth checking out. https://lnkd.in/ePanB6pp--I also provide technical summaries of the latest LLM research and developments here: https://lnkd.in/embzyF3Fâ€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_93": {"text": "RAG for LLMsGreat to see an overview of all the retrieval augmented generation (RAG) research that has been happening.Should be a great read for the end of the year.https://lnkd.in/ezbGUJtq--I also provide technical summaries of the latest LLM research and developments here: https://lnkd.in/embzyF3Fâ€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_94": {"text": "â€œUse LLMs as a reasoning engine to process information, rather than using it as a source of memorized informationâ€Andrew Ng said it best.â€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_95": {"text": "Andrew Ng always has some of the most intuitive ways to explain AI concepts (simple or complex)One of the greatest educators in the AI space.Really enjoying his talk at NeurIPS.â€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_96": {"text": "ðŸŒŸ Big thanks to Dr. Elvis Saravia from DAIR.AI for the wonderful presentation! ðŸ™ŒðŸ‘ Your insights added immense value to the session. #LatinXinAI #NeurISP2023", "name": "omarsar", "source": "Linkedin"}, "Post_97": {"text": "Mistralâ€™s first AI endpoints are here!Things are about to get super interesting in the ecosystem.Whatâ€™s available (from Devendra Chaplot):Three chat endpoints with competitive pricing:Mistral-tiny: Mistral 7B Instruct v0.2, upgraded base model with higher context length 8K --> 32K and better finetuning, 6.84 --> 7.61 on MT Bench.Mistral-small: Mistral 8x7B Instruct v0.1, matches or exceeds GPT-3.5 performance, multilingual.Mistral-medium: Outperforms GPT-3.5 on all metrics, multilingual.All endpoints have a 32K context size!Also announcing Mistral-embed, an embedding model with a 1024 embedding dimension, achieves 55.26 on MTEB.https://lnkd.in/eWRfDuDrâ€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_98": {"text": "How to build custom GPTs using OpenAI functionsHere is a highly recommended webinar! It will discuss how to build custom GPTs using OpenAI Functions and the different applications it enables. The webinar will cover the following:- Engaging with CSV datasets using SingleStoreDB GPT in practical scenarios- Overcoming the limitations of OpenAI's custom GPTs with dynamic data (and larger datasets)- Strategies for effective natural language processing with dynamic datasets- Best practices for scaling AI applications with streaming dataJoin here: https://lnkd.in/d6MKVG6gâ€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_99": {"text": "Excited to be teaching the Prompt Engineering for LLMs course one more time this year.It has been an incredible experience to have taught 200+ students about the most advanced prompting techniques for improving the performance and reliability of LLMs.Today, I am proud to be selected as one of Maven's top courses! The next cohort kicks off in a week so I've partnered with Maven to provide a 25% discount as part of a limited-time offer through Wednesday! If you are interested, join the cohort with this link to get the discount: https://lnkd.in/eNKfQKvhâ€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_100": {"text": "Beginnerâ€™s Guide to GenAI: LLMs, RAG, and moreWe are moving beyond the exploratory phase and into building real-world applications with LLMs. There are several components and tools that stand out when developing with LLMs and generative AI in general. Check out this upcoming webinar on how Large Language Models (LLMs) are shaping the future and the tools/techniques to get familiar with. It will also provide a hands-on guide on Retrieval-Augmented Generation (RAG) and its applications.Join here: https://lnkd.in/eGTASNtnâ€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_101": {"text": "Top ML Papers of the Week (Nov 20 - Nov 26): - Mirasol3B- System 2 Attention- Parallel Speculative Sampling- Advancing Long-Context LLMs- Teaching Small LMs To Reason- LLMs as Collaborators for Medical Reasoning...(paper summaries and links in the article)â€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_102": {"text": "Top ML Papers of the Week (Nov 6 - Nov 12):- S-LoRA- MusicGen- FreshLLMs- Hallucination in LLMs- Rephrase and Respond- Simplifying Transformer Blocks...(paper summaries and links in the article)â€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_103": {"text": "Retrieval APIs for Open Source LLMsBuilding a RAG application is fun but not cheap. You really have to take into account cost, customization, ease of scaling, and the option to work with different LLMs (including open-source ones). Abacus AI recently announced its open-source retrieval APIs. It's probably the most comprehensive offering for building real-world RAG applications with custom LLMs. It's now better understood that RAG and fine-tuning can compliment each other so this is a promising solution that supports this. With their comprehensive platform, you can build powerful apps to chat with your knowledge base within minutes using RAG and/or fine-tuned LLMs. Their fine-tunes support a larger 32K context and are useful for specific enterprise use cases like Q/A and summarization. They report that their open-source APIs are 20x cheaper than GPT-4. This is important for cases where efficient cost and scaling are a priority.It offers the ability:- to easily setup and scale RAG applications- pick any open-source LLM - Llama2, Giraffe, etc - to power your RAG applications- customize chunking, embedding, and retrieval strategies to suit your needsSign up here: https://abacus.ai/ragapiâ€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_104": {"text": "Real-Time AI Threat Detection Using KafkaAnother powerful application of modern deep learning and similarity search is threat detection.This new webinar will show another great example of how to leverage vector embeddings to build a real-world network intrusion detection system.Join here: https://lnkd.in/ecK_KDFTâ€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_105": {"text": "Hallucination in LLMsA comprehensive survey (50+ pages) on hallucination in LLMs.Most of the LLMs and prompting approaches we use today lead to some form of hallucination so it's becoming increasingly important to test for this. The first step is to be aware of and detect these issues and then be familiar with tactics to mitigate them. This paper is a great place to start. Bookmark this one!https://lnkd.in/eywFiyvi---I also provide technical summaries of the latest and most important AI research and developments here: https://lnkd.in/embzyF3Fâ€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_106": {"text": "LLM CollectionBesides having a comprehensive list of prompting techniques for LLMs, we now track some of the most impactful LLMs to date. This is a great place to keep track of the latest general-purpose and domain-specific LLMs. More to come.Think of this as a living LLM survey. We are always open to feedback and improving the guide. You can directly add any missing LLMs as well.We are also working on another set of comprehensive guides on how to explore and build with these impactful LLMs. Currently, we have ChatGPT, GPT-4, and Flan, but more are coming soon (especially on the open-source side of things). Including Llama 2, Llama Code, T5-Flan, and Mistral to name a few.https://lnkd.in/e7UsZwPfâ€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_107": {"text": "Some of the most exciting opportunities in AI today involve building novel apps and interfaces such as LLM-powered knowledge assistants.Building full-stack AI apps requires knowledge of a framework like React and easy access to key features like Chat completion APIs. Recently, I have noticed a lot of libraries starting to support functionalities to build real-time AI apps. This encourages builders to easily and more conveniently leverage LLMs and other vital technologies such as vector databases and embeddings.While still in the exploration phase, the space is moving quickly so it's a good time to upskill on building LLM-powered full-stack apps.If you are interested in understanding how to build full-stack AI apps, check out this new webinar: https://lnkd.in/epDxvhHFâ€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_108": {"text": "LLMs Meet New KnowledgeWhile approaches like RAG are proving effective in enriching LLM workflows, one common issue is knowledge mismatch. LLMs are generally static (with a training cutoff) so injecting new knowledge could create issues with performance and factuality.This paper presents a benchmark to assess LLMs' abilities in knowledge understanding, differentiation, and association. Benchmark results show (unsurprisingly) that an LLM performance when introduced with new knowledge is not satisfactory.Combining LLMs with external information is only getting more popular so it's great to see work like this that provides more insights into this common problem and how to properly measure it. Nice paper to read. (paper in the comments)â€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_109": {"text": "The deeper I go into LLM use cases, the more the need for customization.RAG and finetuned models bridge that gap. But these solutions are not easy to get right. RAG only works if your retriever is effective and finetuning only makes sense if the data quality is good. That being said, I see a lot of synergies with these two approaches for enabling even better customization of LLMs. In other words, these approaches are not mutually exclusive and, in fact, can compliment each other. Example: a finetuned model can get you the right tone/style for a customer success chatbot but it can be complimented with RAG to optimize the context which further improves usability and personalization.This is why I typically advise AI teams to break a task down into smaller subtasks which could enable using a combination of approaches that enrich your LLM-powered solution. Many such cases.â€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_110": {"text": "Instruction Tuning the Largest Pretrained Retrieval-Augmented LLMThis exciting new paper from NVIDIA introduces Retro 48B, the largest LLM pretrained with retrieval.Continues pretraining a 43B parameter GPT model on additional 100B tokens by retrieving from 1.2T tokens (using the Retro augmentation method). The Retro 48B model shows significant perplexity improvement over its GPT 43B counterpart. Scaling the Retro model to 48B means it can be instruction-tuned more effectively. This work applies instruction tuning to Retro 48B and demonstrates significant improvement (+7%) over the instruction-tuned GPT on zero-shot question-answering tasks. The important insight from this work is the potential benefit attained from pretraining with retrieval. Results highlight the promising direction to obtain a better GPT decoder for QA through continued pretraining with retrieval before instruction tuning.(paper in the comments)â€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_111": {"text": "Machine Learning Engineering Guides and ToolsThe notes in this ML guide from @StasBekman are amazing!Includes all kinds of tips on methodologies to better train LLMs and multi-modal models.https://lnkd.in/dW-E28TSâ€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_112": {"text": "A Survey of LLMs for HealthcareThis looks like a nice comprehensive overview of LLMs applied to the healthcare domain. https://lnkd.in/eQGsRgsYâ€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_113": {"text": "Toward Language Agent Fine-tuningExplores the direction of fine-tuning LLMs to obtain language agents. Finds that language agents consistently improved after fine-tuning their backbone language model. Claims that fine-tuning a Llama2-7B with 500 agent trajectories (generated by GPT-4) leads to a 77% HotpotQA performance increase.In general, fine-tuning is an effective way to improve the performance of LLMs on various niche and even complex tasks. In addition, you also get a lot of benefits like increasing efficiency and reducing costs. Language agents, in particular, require the ability to reason and act. This study finds that we can bring the benefits of fine-tuning to language agents which sounds like an intriguing idea, especially because these agents require more controllability and generalization.Paper: https://lnkd.in/eZ4Nrkvj---Are you keen on staying up to date with LLM research? I also provide technical summaries of the latest and most important LLM research and developments here: https://lnkd.in/embzyF3Fâ€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_114": {"text": "Top ML Papers of the Week (Sep 25 - Oct 1): - Boolformer- MentalLlaMa- The Reversal Curse in LLMs- Long-Context Scaling with LLMs- Graph Neural Prompting with LLMs- Vision Transformers Need Registers...(all paper summaries and links in the article)â€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_115": {"text": "New Development in Long-Context LLMs!Meta researchers propose a series of long-context LLMs that support context windows of up to 32K.Builds on continual pretraining from Llama 2 with longer training sequences.Attains significant improvements on long-context tasks over Llama 2. They propose a 70B variant that can already surpass gpt-3.5-turbo-16kâ€™s overall performance on a suite of long-context tasks. This involves a cost-effective instruction tuning procedure that does not require human-annotated long instruction data.Suggests that having abundant long text in the pretraining dataset is not the key to strong performance. Long-context continual pretraining is more efficient. Don't sleep on these long-context LLMs. We know what they are capable of so it's exciting to see more efforts on effective long-context scaling with open-source LLMs!Really great paper to read for the weekend. Bookmark this one!https://lnkd.in/dbrCBhE2â€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_116": {"text": "Technical report on the Qwen LLM series. Another LLM that shows the strength of RLHF on tasks involving tool use and planning capabilities for creating language agents. This release includes chat, math, and code-specialized models.(links in the comments)â€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_117": {"text": "The capabilities of LLMs are no longer confined to text-based interactions. The multimodal AI world is fast approaching.We're entering an era where these models can perceive and interact through multiple modalitiesâ€”seeing, hearing, and speaking.The recent release of OpenAI's ChatGPT and its see, hear, speak capabilities is a good indicator of where things are headed.If you find this evolution as compelling as I do, check out this new webinar that will cover the process of building an application that integrates voice recognition, OpenAI embeddings, and real-time analytics.You'll learn how to fetch dynamic financial data, integrate company news articles, and even generate human-like audio using the latest text-to-speech models.https://lnkd.in/g5Wq4x96â€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_118": {"text": "Chain-of-Verification reduces Hallucination in LLMsDevelops a method to enable LLMs to deliberate on responses to correct mistakes. Steps:1) draft initial response2) plan verification questions to fact-check the draft3) answer questions independently to avoid bias from other responses4) generate a final verified responseForcing the LLM to perform step-by-step reasoning (CoT) already improves the results in many use cases. But it turns out that by being more explicit about verifying questions, you can also decrease hallucination.Good approach to experiment with if you are having problems with hallucinations. (paper in the comments)â€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_119": {"text": "DALLÂ·EÂ 3 is here and it looks amazing!Some thoughts:From the announcement: DALLÂ·EÂ 3 delivers significant improvements over DALLÂ·EÂ 2 when generating text within an image and in human details like hands. DALLÂ·EÂ 3 creates engaging images by defaultâ€”no hacks or prompt engineering required.Very curious to see to what extent we will rely on prompt engineering for this version. That has been one of the pain points for image generation systems, making them hard to use or get good results with. OpenAI claims we won't need as much prompt engineering and that DALLÂ·EÂ 3 adheres better to prompt instructions and details. Also, incredibly excited about the availability of DALLÂ·EÂ 3 in ChatGPT Plus and how this could improve the entire process of prompting and discovering interesting prompts to generate compelling and unique images. https://lnkd.in/eVymV5Wtâ€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_120": {"text": "The Rise and Potential of LLM Based AgentsThis is super cool! A nice repo containing a comprehensive list of papers on LLM based agents. Bookmark this one!https://lnkd.in/dnZk_fJEâ€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_121": {"text": "Language Modeling is CompressionThis is a really interesting paper that evaluates the compression capabilities of LLMs. Furthermore, it investigates how and why compression and prediction are equivalent.Shows that LLMs are powerful general-purpose compressors due to their in-context learning abilities.Finds that Chinchilla 70B, while trained primarily on text, compresses ImageNet patches to 43.4% and LibriSpeech samples to 16.4% of their raw size, beating domain-specific compressors like PNG (58.5%) or FLAC (30.3%), respectively.Provides more insights into scaling laws, showing that the optimal model size is linked to the dataset size and cannot be scaled without limit.https://lnkd.in/eSSdGVKPâ€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_122": {"text": "Asking 60+ LLMs a set of 20 questionsThis is a neat little project testing different LLMs with prompts that test for different capabilities like basic reasoning and instruction following. I have a similar set of tests that I perform on LLMs but my tests are more robust than this list and more catered to real-world use cases. I still like the effort here as I find it useful to do simple tests like this these days, especially with all the LLMs being released.https://lnkd.in/eMZcMjb5â€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_123": {"text": "LLMs as OptimizersThis is a really neat idea. This new paper from Google DeepMind proposes an approach where the optimization problem is described in natural language. An LLM is then instructed to iteratively generate new solutions based on the defined problem and previously found solutions.It was first tested on linear regression and the traveling salesman problem. Leveraging LLMs with simple prompting match or surpass hand-designed heuristic algorithms. This shows good potential for using LLMs as optimizers. The idea is then applied to prompt optimization that aims to maximize task accuracy on different tasks like math word problem-solving.The first piece of the proposed meta-prompt takes in previously generated prompts along with corresponding training accuracies. The second piece includes the optimization problem description with samples obtained from a training set representing the task.At each optimization step, the goal is to generate new prompts that increase test accuracy based on the trajectory of previously generated prompts.The optimized prompts outperform human-designed prompts on GSM8K and Big-Bench Hard, sometimes by over 50%!For math word problem solving, one of the most effective instructions found begins with Take a deep breath and work on this problem step-by-step.I think there is more to unlock here. The tasks this approach was evaluated on were not exactly too diverse in my opinion. I believe it would be more challenging to apply these found prompts to more complex and real-world use cases (article/email/website generation). There is also an opportunity to incorporate human feedback which could require direct prompt engineering expertise. Perhaps finding optimal prompt chains could be a good next step for powering LLM-based agents. There is a lot to explore here.(paper in the comments)â€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_124": {"text": "An Open LLM and How to Train It with a $100K BudgetThis paper introduces a new open LLM called FLM-101B.Claims that  the LLM with 101B parameters and 0.31TB tokens can be trained on a $100K budget.If true, this is amazing! Performance is okay on some of the tasks but I like the overall vision of significantly reducing LLM training cost. This can make model training more accessible and promote LLM research.They analyze different growth strategies, growing the number of parameters from smaller sizes to large ones. They ultimately employ an aggressive strategy that reduces costs by >50%. In other words, three models are trained sequentially with each model inheriting knowledge from its smaller predecessor (16B -> 51B -> 101B) while achieving competitive performance.Model checkpoints were made available as well.Check out the paper for full analysis and insights. Exciting times!(link in the comments)â€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_125": {"text": "ðŸŽ“ML Papers Explained (5K â­ )Paper explanations have now been neatly categorized.The list contains explanations of key concepts and papers in ML, ranging from tabular deep learning to document AI to LLMs. Really cool to see how this project is evolving. Stay tuned for more!Efforts in collaboration with DAIR.AI and Ritvik Rastogi.https://lnkd.in/ecrenHgZâ€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_126": {"text": "Top ML Papers of the Week (August 21 - August 27): - Code Llama- Prompt2Model- Use of LLMs for Illicit Purposes- Survey on Instruction Tuning for LLMs- A Survey on LLM-based Autonomous Agents- Language to Rewards for Robotic Skill Synthesis...(all papers and summaries in the article)â€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_127": {"text": "A Survey on LLM-based Autonomous AgentsNice paper presenting a comprehensive survey of LLM-based autonomous agents.It delivers a systematic review of the space and a summary of various applications of LLM-based AI agents in domains like social science and engineering.(link in the comments)â€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_128": {"text": "If you love building with LLMs and have experience with developing complex prompts, please message me directly.I have a few paid opportunities to work on fun LLM and prompt engineering projects.â€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_129": {"text": "Get Insights From Your Data with Data LLMThis is a neat blog post discussing how to use LLMs to find actionable insights. LLMs are enabling key features like understanding user query intent, fetching knowledge, generating search queries, and summarizing results. All these components require deep expertise as you aim to build with complex solutions like data warehouses. With Data LLM (from Abacus.AI), it's possible to leverage the capabilities of LLMs to interact with data warehouses to support all kinds of use cases. For instance, product managers or marketers can instantly pull customer data reports such as â€œNumber of signups by the day of the week in the USâ€, â€œNo of customers who churned at a regional levelâ€, etc.The Data LLM Agent is given access to your data warehouse and it extracts metadata about tables and stores it in a document retriever. Different embedding strategies are available to store the vectors in the document retriever.That is fairly easy to set up with the Abacus platform with security in mind. A user can now interact with the AI-powered Data LLM Agent which performs a few steps to carry out the user query. The user query is used to retrieve relevant tables and metadata. The agent leverages an LLM (e.g., Claude, GPT-4, or Llama 2) to form the SQL query that will be used to query the data warehouse. The data that's returned is then summarized using another LLM with the goal of answering the question asked by the user.You can deploy and engage with the Data LLM Agent through the provided dashboard or set up bots that communicate with other platforms like Teams and Slack. Abacus also provides API endpoints to power other apps or even enable agents to talk to each other. This is a powerful solution that enables companies to leverage their data in creative ways. LLMs have the power to simplify interfaces and what you can do with data which allows companies to be more productive with tedious tasks like customer support.Read more here: https://lnkd.in/ee5inSxEAs with other previous posts, if there is enough interest, I will also be posting a full demo with a use case showcasing this powerful solution. Stay tuned!â€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_130": {"text": "Teach LLMs to PersonalizeLLMs are already good at synthesizing text but personalized text generation can unlock even more. Customizing style is essential for a lot of domains like personal communication, dialogue, marketing copies, stories, etc. It's actually hard to do this with the current LLMs via pure prompt engineering or custom instructions.This new paper proposes a general approach for personalized text generation using LLMs. The goal is to have an LLM generate personalized text without relying on predefined attributes.There are lots of interesting implementation details in the paper including the use of multitask learning to improve personalization in LLMs. The authors mention that their multistage framework is inspired by writing education. There is no secret sauce it's more a matter of tuning and integrating the right components (e.g., retriever and ranker) to achieve personalized text generation. This paper highlights how we can take inspiration from how humans achieve tasks and apply it to LLMs. It also highlights the important role of all these components such as retrievers to go beyond basic text generation.(paper link in the comments)â€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_131": {"text": "Model Compression for LLMsNice short survey on the recent model compression techniques for LLMs. A quick way to get a high-level overview of topics such as quantization, pruning, knowledge distillation, and more. It also provides an overview of benchmark strategies and evaluation metrics for measuring the effectiveness of compressed LLMs.Great resources for both researchers and practitioners.(paper link in the comments)â€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_132": {"text": "Using GPT-4 Code Interpreter to Boost Mathematical ReasoningThis paper proposes a zero-shot prompting technique for GPT-4 Code Interpreter that explicitly encourages the use of code for self-verification which further boosts performance on math reasoning problems. Self-verification is already a powerful approach to enhance the performance of LLMs on many tasks but this approach leverages the evaluation of code execution which could make it interesting to solve other kinds of problems. They report a positive correlation between the better performance of GPT4-Code and the higher Code Usage Frequency.Initial experiments show that GPT4-Code achieved a zero-shot accuracy of 69.7% on the MATH dataset which is an improvement of 27.5% over GPT-4â€™s performance (42.2%). Lots to explore here. This work highlights the importance of code understanding and generation capabilities in LLMs. Some of the ideas presented in this paper (specifically, the code-based self-verification and verification-guided weighted majority voting technique) can lead to building high-quality datasets that could potentially help improve the mathematical capabilities in open-source LLMs like Llama 2. https://lnkd.in/eXCA-CHzâ€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_133": {"text": "Information Theory, Inference, and Learning AlgorithmsThis is hands down one of the best books you can spend time on if you are studying CS or ML. It unifies ML and information theory in an elegant way. FREE PDF available.Book: https://lnkd.in/d8EVQCM8 Video lectures: https://lnkd.in/dJhMQZiHâ€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_134": {"text": "I work on a lot of NLP projects and it's starting to feel like I do more prompting than actual coding. LLMs might completely change the way we code as they become more integrated into our tools and workflows. Still doing a lot of stitching with tools like ChatGPT and Copilot but I expect more seamlessness as these tools provide more functionalities like function calling.That being said, there might need to be more innovation around interfaces/UI/UX than can enables this. What we have today feels disconnected and chat is probably not it. Exciting times to make groundbreaking discoveries and innovations with LLMs.â€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_135": {"text": "You can now connect Jupyter with LLMs! It provides an AI chat-based assistant within the Jupyter environment that allows you to generate code, summarize content, create comments, fix errors, etc.You can even generate entire notebooks using text prompts!You can also pass it documentation which it can use as a knowledge source to answer questions. It uses RAG it seems.There is also %%ai magic commands available to connect to different LLM within the cells and pass prompts that generate code directly in the notebooks.Seems like LangChain is used to power these new AI-powered capabilities. Supported models include LLMs from OpenAI, AI21, Anthropic, Cohere, and more. Local models are also possible to use. This is really cool stuff!(blog announcement in the replies)â€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_136": {"text": "Meta AI is open-sourcing AudioCraft, a multi-purpose framework for generating music and sounds and enabling compression capabilities.AudioCraft contains training and inference code for a series of models, including MusicGen, AudioGen, and EnCodec. This is going to allow others in the community to extend these models to all sorts of use cases or research problems.This release is exciting as it simplifies building on top of the state-of-the-art in audio generation. People can now build things like sound generators and compression algorithms with the same code base.(links in the comments)â€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_137": {"text": "Introducing Giraffe. A new open long-context LLM.Giraffe is a Llama 1 finetune that extends context lengths to 4K and 16K.There is a huge interest in LLMs that support longer contexts. From a practical perspective, this is important as you can do more with longer-context LLMs such as incorporating more information from knowledge bases to support your custom LLM chat applications. Thanks to Abacus.AI for releasing their models and evaluation datasets for research purposes.The Llama 2 versions are coming soon!code and results: https://lnkd.in/evkN6r-6blog: https://lnkd.in/eGzYmcUdâ€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_138": {"text": "How is ChatGPTâ€™s behavior changing over time?If you are developing with LLMs or in this case GPT-3.5 or GPT-4, it's definitely worth taking a look at this report. There is suspicion in the AI community that models like GPT-4 are changing/degrading in performance and behavior. The paper finds that the performance and behavior of both GPT-3.5 and GPT-4 vary significantly across these two releases and that their performance on some tasks have gotten substantially worse over time.There are no official reports on this matter but it highlights the importance of things like continuous monitoring of LLM output quality.Strongly recommend reading the report. paper: https://lnkd.in/e_pcZ8gtrepo: https://lnkd.in/epF-a_mVâ€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_139": {"text": "JUST IN: Meta AI introduces Llama 2Itâ€™s been ~4 months since Llama was launched and took the community by storm.This new release brings a few new things.Here is the breakdown:- The release involves a partnership with Microsoft- Available for research and commercial use- Releases pretrained and fine-tuned models (7B, 13B, and 70B)- Pretrained on 40% more data than Llama 1- Improvements to architecture included- Safer and higher-quality models using supervised fine-tuning and RLHF- Models available in Azure or for direct downloadproject: https://ai.meta.com/llama/paper: https://lnkd.in/dk9NdaQ7â€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_140": {"text": "looking to connect with more LLM researchers and developers hereif thatâ€™s you, reply with hi ðŸ‘‹also feel free to share what you are working onâ€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_141": {"text": "Top ML Papers of the Week (July 10 - July 16): - CM3Leon- LongLLaMA- AnimateDiff- Patch nâ€™ Pack: NaViT- Secrets of RLHF in LLMs- LLMs as General Pattern Machines...(all paper summaries and links in the article)â€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_142": {"text": "Top ML Papers of the Week (June 26 - July 2):- LeanDojo- DragDiffusion- Scaling MLPs- Visual Navigation Transformer- GPT-4 for Programming Education- Extending Context Window of LLMs...All summaries and papers in the article below:â€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_143": {"text": "LLM Powered Autonomous AgentsThis is an incredible overview of LLM-powered autonomous agent systems. Includes case studies and proof-of-concept examples. https://lnkd.in/e4g9WJucâ€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_144": {"text": "AI agents are exciting but how and what can you use them for? You can use them for:- summarization- custom chat- code translation- content creation- insights generationHere is an impressive solution for this:Abacus.AI AI agents are powered by LLMs and you can build and deploy them quickly. Some building blocks supported in the platform include data transformation, data store, user code, prompt API, and vector store.You can chain these blocks to create powerful AI agents.You can also leverage any LLM (GPT-4 or Abacus fine-tuned models).They provide all these fundamental building blocks to build creative and useful AI agents that automate workflows.Examples of tasks that AI agents can execute include: - summarize contracts or docs- analyze dashboards and send weekly emails- choose which model to run for a particular task- chain multiple ML models togetherYou can unlock huge value by combining LLMs with external tools & knowledgebases.AI agents enable that by combining the synthesizing capabilities of LLMs with high-quality data.Now you can easily deploy AI agents and monitor usage.Try it: https://lnkd.in/dgK8Wfv5â€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_145": {"text": "Extending Context Window of LLMsThis work extends the context window of LLMs like LLaMA to up to 32K. All done with minimal fine-tuning (within 1000 steps). Very promising!Previous methods for extending context window are inefficient so it's interesting to see this new approach attain good performance on several tasks while being more efficient and cost-effective. https://lnkd.in/e7tQSZSqâ€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_146": {"text": "FinGPT: Open-Source Financial LLMsFinGPT is an open-source LLM for the finance sector. It takes a data-centric approach, providing researchers & practitioners with accessible resources to develop FinLLMs.paper: https://lnkd.in/eYUTCaYgcode: https://lnkd.in/eqwvCGayâ€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_147": {"text": "Top ML Papers of the Week (June 5-11):- AlphaDev- MusicGen- Fine-Grained RLHF- Humor in ChatGPT- Concept Scrubbing in LLM- Augmenting LLMs with Databases...(links to all papers in the article)â€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_148": {"text": "Awesome Multimodal LLMsIf you want to find recent multimodal LLM papers, this is a great place to start.https://lnkd.in/gQC3t9f8â€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_149": {"text": "Wolfram Prompt RepositoryA collection of ~200 basic and advanced LLM prompts for accomplishing tasks that range from fun to technical.They are specific to Wolfram but there are a lot of interesting ideas on how prompts can be used and designed.https://lnkd.in/gEbJa3rtâ€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_150": {"text": "BiomedGPT, a unified biomedical generative pretrained transformer model for vision, language, and multimodal tasks.Achieves state-of-the-art performance across 5 distinct tasks with 20 public datasets spanning over 15 unique biomedical modalities. https://lnkd.in/etUwHUjC#machinelearning #ai #llmsâ€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_151": {"text": "Top ML Papers of the Week (May 22 - 28): - LIMA- QLoRA- Gorilla- Voyager- LLM Research Directions- Reinventing RNNs for the Transformer Era...(all links and papers in the article)#machinelearning #deeplearning #aiâ€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_152": {"text": "Introducing Falcon-40B. A new language model trained on 1000B tokens.What's included:- 7B and 40B models made available by TII- surpasses LLaMA 65B and other models like MPT and RedPajama on the Open LLM Leaderboard- architecture is optimized for inference, with FlashAttention and multiquery- Instruct model available- license allows personal and research use and commercial use with limitationshttps://lnkd.in/eyp-rGBh#machinelearning #deeplearning #aiâ€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_153": {"text": "This maths book is trending on Hacker News!I took a quick look and realized how great of a book this is to learn how to think mathematically. It's 700 pages long and very approachable compared to other maths books. Maths has been fundamental in my journey as a machine learning researcher. So I highly recommend books like this. https://lnkd.in/e9DvfxAy#machinelearning #mathsâ€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_154": {"text": "Guidance - enables prompting LLMs more effectively and efficiently than traditional prompting or chaining. This promising prompting tool from Microsoft combines rich templating and logical control flow with LLMs. It allows the interleaving of generation and prompting for clear and parsable results.This looks really useful for increasing the efficiency of designing prompt templates and improving the reliability and accuracy of results.It also has a lot of potential for improving inference performance. In the example below, notice how you can combine the desired structure and how and where you want the content to be generated.https://lnkd.in/e78XtgTP#machinelearning #deeplearning #ai #llmsâ€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_155": {"text": "Text Classification with LLMs- introduces a prompting technique to improve text classification with LLMs- leverages fine-tune models to search for demonstrations- new SOTA performances on 4 out of 5 widely-used text-classification benchmarksOverall, the paper provides/discusses good ideas for how to improve prompts for text classification.https://lnkd.in/dGvf3tgg#machinelearning #deeplearning #ai #llmsâ€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_156": {"text": "Brex's Prompt Engineering GuideA prompt engineering guide with lessons learned from researching and creating LLMs for production use cases. This is an impressive resource!This is exactly how I started my own prompt engineering guide. Good to see others sharing in the open too!This is super encouraging! I am pushing a more comprehensive Prompt Engineering Guide in the coming weeks too. Super excited about that. #machinelearning #deeplearning #aiâ€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_157": {"text": "Top ML Papers of the Week (May 8 - 14): - PaLM 2- ImageBind- InstructBLIP- StarCoder- MultiModal-GPT- LLM explains neurons in LLMs...(summaries and paper links below)#machinelearning #deeplearning #aiâ€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_158": {"text": "ChatGPT-powered tool for code explanation- GPTutor is a ChatGPT-powered programming tool for code explanation provided as a VSCode extension- claims to deliver more concise and accurate explanations compared to vanilla ChatGPT and Copilot- performance and personalization enhanced via prompt engineering; programmed to use more relevant code in its promptsAs for trends, it's becoming clear that you can improve results for personalized tutoring using more advanced and optimized prompts that leverage user preferences and richer context.This type of personalized LLM-powered programming tool could be a game-changer in computer science education and education more broadly. (link in the comments)#machinelearning #deeplearning #ai #llmsâ€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_159": {"text": "Long-Range Transformers with Unlimited Length Input- augments pre-trained encoder-decoder transformer with external datastore to support unlimited length input- shows usefulness in long-document summarization and can improve models like Longformer without additional learned weights- could potentially be used to improve the performance of retrieval-enhanced LLMspaper: https://lnkd.in/diVzvdy9code: https://lnkd.in/d9n7Yn6w#machinelearning #deeplearning #aiâ€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_160": {"text": "Enabling LLMs to Reason and Memorize with Self-Notes- allows LMs to deviate from the input sequence at any time to explicitly think- this enables the LM to recall information and perform reasoning on the fly- can act as a recurrent memory as the Self-Note answers are fed back to the model- experiments show that this method scales better to longer sequences unseen during trainingBenefits (as extracted from the paper):- The model can use notes as a form of working memory by writing information that might be useful in the future.- It can also use a note as an intermediate reasoning step by inferring new facts as it reads; It can ask a question and answer it within it, useful in multistep reasoning where a final question requires answering multiple sub-questions.https://lnkd.in/gdD8enWe#machinelearning #deeplearning #ai #llmsâ€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_161": {"text": "A Review of ChatGPT ApplicationsA review of ChatGPT applications in education, marketing, software engineering, and healthcare.Includes discussion around benefits, drawbacks, and research directions.https://lnkd.in/e7w3uCWS#machinelearning #deeplearning #ai #llms #chatgptâ€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_162": {"text": "Great overview of ChatGPT applications, opportunities, and threats. https://lnkd.in/eGTB_GCT#machinelearning #deeplearning #aiâ€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_163": {"text": "ImpressionGPT - LLMs for Radiology Report SummarizationImpressionGPT leverages the in-context learning capabilities of LLMs for radiology report summarization. It does this by constructing dynamic prompts using domain-specific data. An iterative optimization method, involving automatic evaluation, is used to improve generated results. This work shows the effectiveness of combining advanced prompt engineering techniques and ChatGPT for specific domains like radiology.arxiv.org/abs/2304.08448#machinelearning #deeplearning #ai #llmsâ€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_164": {"text": "Open-source RLHF implementations are on the rise!Challenges to keep an eye on include efficiency, reducing costs, safer LLMs, and collecting high-quality human feedback. Read on below to learn more:#machinelearning #deeplearning #aiâ€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_165": {"text": "Meta AI just released DINOv2!This is a massive release! Here's a breakdown of what you need to know:- DINOv2 is a new method for training high-performance computer vision models based on self-supervised learning.- DINOv2 enables learning rich and robust visual features without supervision which are useful for both image-level visual tasks and pixel-level tasks. Tasks supported include image classification, instance retrieval, video understanding, depth estimation, and much more.- The big deal here seems to be self-supervised learning and how it enables DINOv2 to be used to create general, multipurpose backbones for many types of computer vision tasks and applications. The model generalizes well across domains without fine-tuning. This is self-supervised learning at its finest! - Another important aspect of this research is the composition of a large-scale, highly-curated, and diverse pertaining dataset to train the models. The dataset includes 142 million images.- Other algorithmic efforts include dealing with the instability that arises from training larger models, including more efficient implementations that reduce things like memory usage and hardware requirements. - DINOv2 pretrained models are also released! Pretraining code and recipe for Vision Transformer models are included, including checkpoints for ViT models made available via PyTorch Hub. There is a lot more to unpack from this release and research including all the quantitative and qualitative results, bias analysis, data curation, the carbon footprint of the project, etc. Highly recommend checking out all the links below and the paper.(links in the comment)#machinelearning #deeplearning #aiâ€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_166": {"text": "Everyone is talking about RLHF and how great it works for LLMs like GPT-4.But there are limitations. Incorporating more natural language feedback into LLMs shows promising results. This new paper shows one of the more interesting ideas I have recently come across:#machinelearning #deeplearning #aiâ€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_167": {"text": "Good overview of applications of ChatGPT and GPT-4.The analysis is done on 194 relevant papers and discusses capabilities, limitations, concerns, and more.A great read for AI developers and researchers.https://lnkd.in/eU7r6a5x#machinelearning #deeplearning #aiâ€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_168": {"text": "Prompt Engineering Guide (20K â­ )We started with basic prompt examples and have expanded to a comprehensive prompt engineering guide used by thousands of AI developers and researchers working with LLMs.- Now over 200K+ learners- Chinese & Japanese translations are now available- GPT-4 & ChatGPT guides and notebooks- Collection of all the latest tools and papers on prompt engineering- Added LLM collection- Papers explanations in progress... and much moreWe aim to build the ultimate resource to learn how to work and build with LLMs. A lot more to come. Support and contributions are welcome!web: https://promptingguide.airepo: https://lnkd.in/eXGtDsiBâ€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_169": {"text": "LLMs and Code GenerationOne of the fascinating capabilities of LLMs today is code generation.This new paper proposes an algorithm to improve code generation through LLMs and human-written feedback incorporated at training time. Really cool stuff!#machinelearning #deeplearning #aiâ€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_170": {"text": "ChatGPT is supporting plugins!Things are about to get really interesting with ChatGPT! Here is what's coming:- support for browsing and retrieving information from the internet- a Python code interpreter that helps with mathematical problems, data analysis, visualization, and much more- an open-source retrieval plugin allowing you to leverage your data sources such as files and notes- third-party plugins such as Wolfram, OpenTable, Instacart, and more.https://lnkd.in/ewvRUYHW#machinelearning #deeplearning #aiâ€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_171": {"text": "Sparks of Artificial General Intelligence: Early experiments with GPT-4If you are looking for fun and interesting prompts to try on GPT-4, look no further. This paper has lots of demonstrations for a wide range of tasks and domains.https://lnkd.in/ebEmBEWn#machinelearning #deeplearning #aiâ€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_172": {"text": "GitHub introduces GitHub Copilot XHere is all you need to know:- GitHub Copilot will support GPT-4 - It will provide a ChatGPT-like experience in your editor with chat functionalities (GitHub Copilot Chat)- Copilot for pull requests- GitHub Copilot for Docs to answer questions about documentation- Copilot for the CLI#machinelearning #deeplearning #aiâ€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_173": {"text": "Capabilities of GPT-4 on Medical Challenge ProblemsShows that GPT-4 exceeds the passing score on USMLE by over 20 points and outperforms GPT-3.5 as well as models specifically fine-tuned on medical knowledge (Med-PaLM, a prompt-tuned version of Flan-PaLM 540B).Paper: https://lnkd.in/eCRbzV-r#machinelearning #deeplearning #aiâ€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_174": {"text": "An Overview of Language Models Nice overview of language models covering recent developments and future directions. It also covers topics like linguistic units, structures, training methods, evaluation, and applications.https://lnkd.in/eUZkezBz#machinelearning #deeplearning #aiâ€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_175": {"text": "Top ML Papers of the Week (Mar 6 - Mar 12):- Visual ChatGPT- PaLM-E- A History of Generative AI- MathPrompter- Foundation Models for Decision Making- GigaGAN...#machinelearning #deeplearning #aiâ€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_176": {"text": "MathPrompter: a technique that improves LLM performance on mathematical reasoning problems.It uses zero-shot chain-of-thought prompting and verification to ensure that generated answers are accurate.This is a really interesting paper proposing steps to ensure that the LLM responses are more reliable. The technique could probably be adapted to other tasks where there is a need for precise outputs.https://lnkd.in/dR49KenE#machinelearning #deeplearning #aiâ€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_177": {"text": "Microsoft's Visual ChatGPT is awesome!It connects ChatGPT and different visual foundation models to enable users to interact with ChatGPT beyond language format.Code: https://lnkd.in/efi-qksxPaper: https://lnkd.in/eFNi5JWQ#machinelearning #deeplearning #aiâ€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_178": {"text": "Where I track ML trends:â€¢ Papers with Code - for trending papers + codeâ€¢ Twitter - for discussion of trendsâ€¢ GitHub Trending - for trending ML projects--I am sure there are other great resources out there but these do the job for me.#machinelearning #deeplearning #aiâ€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_179": {"text": "JUST IN: Meta AI introduces LLaMA, a 65B parameter LLM.LLaMa only relies on publicly available data and outperforms GPT-3 on most benchmarks despite being 10x smaller.The MMLU results are quite impressive! Still not as performant as PaLM 540B but you can see the performance gap is being closed. Instruction-tuned LLaMA outperforms other LLMs like Flan-PaLM and Chinchilla.Paper: https://lnkd.in/euzAdGZUApply for access: https://lnkd.in/eemgUhBgRepo link: https://github.com/facebookresearch/llama#machinelearning #deeplearning #aiâ€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_180": {"text": "If you are building with LLMs, it's good to know all the novel adversarial prompting techniques.This paper presents an analysis of the topic. It also discusses attack vectors when augmenting LLMs with retrieval and API calling abilities.https://lnkd.in/eeAuUmJ7#machinelearning #deeplearning #aiâ€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_181": {"text": "If you are using or developing on top of LLMs it's good to know their strengths but it's also equally important to understand their limitations. This paper looks at the limitations and misuse of ChatGPT in education and research. Main findings in the paper: - ChatGPT performs well for structure tasks such as code translation and explaining well-known concepts- ChatGPT struggles with more nuanced tasks, such as explaining less widely-known terms and creating code from scratchOverall, ChatGPT is great at general and common tasks but lacks performance in more nuanced tasks. This is expected given how and what it is trained on. One of the strengths of ChatGPT is fitting human preferences. However, I expect future iterations to get better at more technical explanations which will be very advantageous and increase productivity for practitioners, educators, and researchers.https://lnkd.in/ehU6yztg#machinelearning #deeplearning #aiâ€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_182": {"text": "A Categorical Archive of ChatGPT FailuresComprehensive analysis of ChatGPT failures for categories like reasoning, factual errors, maths, and coding.If you are developing with LLMs it's important to know these failures. Good to see them documented.https://lnkd.in/eXCuaPNf#machinelearning #deeplearning #aiâ€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_183": {"text": "BREAKING: Microsoft announces new AI-powered Bing and Edge. Your copilot for the webBing is running on a new, next-generation OpenAI large language model that is more powerful than ChatGPT and customized specifically for search. It takes key learnings and advancements from ChatGPT and GPT-3.5 â€“ and it is even faster, more accurate and more capable.Note that it's a large language model customized specifically for search. Custom LLMs are taking off this year. This not only applies to search but to other areas such as science, health, education, and so on.https://lnkd.in/edAtkp_P#machinelearning #deeplearning #aiâ€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_184": {"text": "BREAKING: Google announces Bard, its own conversational AI service powered by LaMDA.Bard seeks to combine the breadth of the worldâ€™s knowledge with the power, intelligence and creativity of our large language models.2023 is delivering!Google is probably in the best position to achieve absolutely mindblowing things with LLMs. Easier said than done!In a world with increasing competition and more capable LLMs, execution matters. Curious to test this out at some point.https://lnkd.in/eEC989nm#machinelearning #deeplearning #aiâ€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_185": {"text": "Top ML Papers of the Week (Jan 30 - Feb 5):- REPLUG- SceneDreamer- The FLAN collection- Distractability of LLMs- Blind navigation agents- Mathematical capabilities of ChatGPT...#machinelearning #deeplearning #aiâ€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_186": {"text": "ðŸ™ AI Product IndexWe (DAIR.AI) introduce the AI Product Index to track and organize cool new AI products that are accelerating creativity, productivity, science, and other areas. https://lnkd.in/egkncgXMWe are starting with an index but will be building more on top of this, including guides and other forms of content, to help devs and creators quickly find tools and solutions for their applications. More soon!#machinelearning #deeplearning #aiâ€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_187": {"text": "ðŸ™ Deep Learning Tuning PlaybookAmazing resource released by Google. It shows tips on how to conduct DL experiments and get good results.I think it's also super useful to design exercises and practice more with DL models.#machinelearning #deeplearning #aiâ€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_188": {"text": "Let's build GPT: from scratch, in code, spelled out.This new lecture by Andrej Karpathy is an absolute must-watch.I was waiting for a GPT from scratch lecture with this level of detail. I will be writing notes for this and releasing them soon.https://lnkd.in/eWWviueC#machinelearning #deeplearning #aiâ€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_189": {"text": "The Illustrated Stable DiffusionIf you are curious to learn about how stable diffusion works, @JayAlammar's illustrated guide is a good place to start.blog: https://lnkd.in/espvVC23video: https://lnkd.in/ePEKP_wg#machinelearning #deeplearning #aiâ€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_190": {"text": "Tune-A-Video: One-Shot Video GenerationA text-to-video model to generate videos from text prompts via an efficient one-shot tuning of pre-trained text-to-image diffusion models.Here is my quick summary:Tune-A-Video leverages pre-trained text-to-image diffusion models extending spatial self-attention to spatio-temporal cross-frame attention with an efficient tailored Sparse-Causal Attention module.The current paradigm relies on large-scale text-video datasets for fine-tuning which is computationally expensive. In contrast, only a single text-video pair is provided for training the proposed open-domain T2V generator.Tune-A-Video consistently outperforms CogVideo a 9.4B parameters which is around 6Ã— larger than the proposed model.Overall, this is a great paper proposing the feasibility of one-shot video generation with impressive results. I also like the idea that we are already focusing on computational efficiency for T2V.Paper: https://lnkd.in/etKGFmRpWebsite: https://lnkd.in/exAmgAdQ#machinelearning #deeplearning #aiâ€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_191": {"text": "NEW: Meta AI introduces OPT-IML, a large language model (175B) fine-tuned on 2000 NLP tasks. Uses instruction-tuning to improve zero-shot and few-shot generalization abilities.Here is my quick summary of the paper â†“It presents a large benchmark called OPT-IML for instruction meta-learning of 2K NLP tasks. The goal is to use the framework to find insights about instruction-tuning decisions applied to OPT-30B. The insights are then used to instruction-tune large versions of OPT (30B & 175B).The paper provides an interesting analysis of the effects of instruction-tuning and reports tradeoffs and best practices. Results are reported for different sampling strategies during tuning, scaling laws wrt to tasks & categories, fine-tuning with task demonstrations, and more.As an example of the experiments conducted, the authors report results of scaling the number of training tasks on each generalization level (fully held-out, partially supervised, and fully supervised) for the OPT-IML 30B model under both 0-shot and 5-shot settings. Results show that the most improvements are seen in the fully held-out & partially supervised tasks. Fully supervised tasks' performance remains mostly unchanged, which is desirable. The best settings observed in these experiments are used to effectively train large OPT models (30B & 175B).Overall, there is an entire section in the paper demonstrating that the instruction-tuned models can obtain significant improvements over untuned models on both zero- and few-shot settings on multiple benchmarks (e.g., PromptSource & FLAN) and different standard NLP tasks.Not all is perfect and there are limitations in this work. For example, results show that the large OPT-IML models are competitive on RAFT but lag behind other models like FLAN-PaLM and instruction-tuned GPT-3 models (*davinci) on MMLU and Big Bench Hard.Instruction-tuning continues to show promising results so we can expect to see more work in this area. Models and paper here: https://lnkd.in/euyjyCXX#machinelearning #deeplearning #aiâ€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_192": {"text": "Here is an interesting new paper on multimodal LMs â†“It presents MultiInstruct, a new multimodal instruction tuning benchmark dataset. It's used to fine-tune a unified OFA model, with instruction tuning, to improve multi-modal zero-shot performance on various unseen multimodal tasks.https://lnkd.in/eyMbEvxc#machinelearning #deeplearning #aiâ€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_193": {"text": "Scalable Diffusion Models with TransformersIntroduces Diffusion Transformers... replaces convolutional U-Net backbone with a Transformer giving way to a new class of diffusion models with good scaling properties  and SoTA performance. webpage: https://lnkd.in/eTzvGWhKpaper: https://lnkd.in/eDrkuT8bcode: https://lnkd.in/ecZKSYJp#machinelearning #deeplearning #aiâ€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_194": {"text": "ðŸ™  TorchScale - A Library for Transformers at (Any) ScaleTorchScale is a PyTorch library to scale up Transformers efficiently and effectively. Something worth checking out if you are working on scaling Transformer models.https://lnkd.in/eSXhAZ4j#machinelearning #deeplearning #aiâ€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_195": {"text": "ðŸŽ‰ Proud and excited to announce Galactica - a large language model for science.We trained a 120B parameter language model on a massive scientific corpus that performs different tasks such as solving math problems and summarizing academic literature. Try it out here: https://galactica.org/â€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_196": {"text": "It's so much fun building tiny or minimal versions of machine learning models.There is no standard way to go about it but that's what's so interesting about the process. You learn a lot because you push yourself to explore, test, and question design decisions.This makes a great project for ML students too.I find the exercise useful when wanting to understand the inner workings of a new model.A good public example of this is the minGPT repo by Andrej Karpathy: https://lnkd.in/e77ZeifxI'm also working on some examples and will publish them soon, including my approach.#machinelearning #deeplearning #aiâ€¦more", "name": "omarsar", "source": "Linkedin"}, "Post_197": {"text": "ZerO Initialization: Initializing Neural Networks with only Zeros and OnesInteresting results using a deterministic initialization scheme. It basically initializes the weights of networks with only zeros and ones (up to a normalization factor). Initialization is a very important topic when training neural networks so it's good to see working alternatives. https://lnkd.in/e_-aD7AN#machinelearning #deeplearning #aiâ€¦more", "name": "omarsar", "source": "Linkedin"}}}